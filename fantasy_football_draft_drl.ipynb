{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to add year variability and add some variability around who is available (i.e. drafts don't perfectly follow ADP, some people slip and some positions are over and under targeted in certain drafts, move the ADP cutoff up and down randomly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import random\n",
    "\n",
    "from stable_baselines3 import DQN, PPO, A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# uncomment when SB3 & wandb are working together again\n",
    "# import wandb\n",
    "# from wandb.integration.sb3 import WandbCallback\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_map = {0: 'QB', 1: 'RB', 2: 'WR', 3: 'TE'}\n",
    "\n",
    "class FantasyFootballEnv(gym.Env):\n",
    "    def __init__(self, teams=12, rounds=7, year=None, first_round_pick=None, data_first_year=2018, data_last_year=2022\n",
    "                , apply_penalty=False, penalty_amt=-10):\n",
    "        super(FantasyFootballEnv, self).__init__()\n",
    "        \n",
    "        # Action space: 0: QB, 1: RB, 2: WR, 3: TE\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        \n",
    "        self.observation_space = spaces.Box(low=0, high=rounds, shape=(5,), dtype=np.int32)\n",
    "        \n",
    "        self.data_first_year = data_first_year\n",
    "        self.data_last_year = data_last_year\n",
    "        self.teams = teams\n",
    "        self.rounds = rounds\n",
    "        if year is not None:\n",
    "            self.year = year\n",
    "        else:\n",
    "            self.year = random.randint(2018, 2022)\n",
    "        if first_round_pick is not None:\n",
    "            self.first_round_pick = first_round_pick\n",
    "        else:\n",
    "            self.first_round_pick = random.randint(1, teams)  \n",
    "        self.pick = self.first_round_pick\n",
    "            \n",
    "        self.player_df = self.create_player_df()\n",
    "        \n",
    "        self.flex_count = 0\n",
    "        \n",
    "        self.apply_penalty = apply_penalty\n",
    "        self.penalty_amt = penalty_amt\n",
    "        \n",
    "        # Other initializations\n",
    "        self.current_round = 1\n",
    "        self.roster = {'QB': [], 'RB': [], 'WR': [], 'TE': []}\n",
    "        self.position_counts = {\n",
    "            'QB': 1,\n",
    "            'RB': 2,\n",
    "            'WR': 2,\n",
    "            'TE': 1,\n",
    "            'FLEX': 1\n",
    "        }\n",
    "        self.flex_positions = ['RB', 'WR', 'TE']\n",
    "        self.drafted_players = []\n",
    "        \n",
    "        # print(f'Year: {self.year}, First Round Pick: {self.first_round_pick}, Teams: {self.teams}, Rounds: {self.rounds}')\n",
    "        \n",
    "    def reset(self, seed=None, year=None, first_round_pick=None):\n",
    "        \n",
    "        # https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/#sphx-glr-tutorials-gymnasium-basics-environment-creation-py\n",
    "        # We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        if year is not None:\n",
    "            self.year = year\n",
    "        else:\n",
    "            self.year = random.randint(2018, 2022)\n",
    "        if first_round_pick is not None:\n",
    "            self.first_round_pick = first_round_pick\n",
    "        else:\n",
    "            self.first_round_pick = random.randint(1, self.teams)\n",
    "        self.pick = self.first_round_pick\n",
    "            \n",
    "        self.flex_count = 0\n",
    "\n",
    "        self.current_round = 1\n",
    "        self.roster = {'QB': [], 'RB': [], 'WR': [], 'TE': []}\n",
    "        self.drafted_players = []\n",
    "        \n",
    "        # Create the initial observation with the current round and counts for each position\n",
    "        observation = [self.current_round, len(self.roster['QB']), len(self.roster['RB']), len(self.roster['WR']), len(self.roster['TE'])]\n",
    "                \n",
    "        # https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/#sphx-glr-tutorials-gymnasium-basics-environment-creation-py\n",
    "        info = {}\n",
    "        \n",
    "        # print(f'Year: {self.year}, First Round Pick: {self.first_round_pick}, Teams: {self.teams}, Rounds: {self.rounds}')\n",
    "\n",
    "                \n",
    "        return observation, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        \n",
    "        penalty = False\n",
    "        done = False\n",
    "        reward = 0\n",
    "        \n",
    "        selected_position = position_map[action]\n",
    "        self.pick = self.snake_draft_pick(self.teams, self.current_round, self.first_round_pick)\n",
    "        selected_player, selected_player_points = self.draft_player(selected_position)\n",
    "        self.roster[selected_position].append(selected_player)\n",
    "        \n",
    "        pos_max_exceeded = len(self.roster[selected_position]) > self.position_counts[selected_position]\n",
    "        flex_pos_selected = selected_position in self.flex_positions\n",
    "        if pos_max_exceeded & flex_pos_selected:\n",
    "            self.flex_count += 1\n",
    "            \n",
    "        if self.apply_penalty:\n",
    "            flex_max_exceeded = self.flex_count > self.position_counts['FLEX']\n",
    "            if (pos_max_exceeded & flex_pos_selected & flex_max_exceeded) | (pos_max_exceeded & ~flex_pos_selected): penalty = True\n",
    "            if penalty: reward += self.penalty_amt\n",
    "        \n",
    "        observation = [self.current_round, len(self.roster['QB']), len(self.roster['RB']), len(self.roster['WR']), len(self.roster['TE'])]\n",
    "        \n",
    "        if self.current_round >= self.rounds:\n",
    "            done = True\n",
    "            total_points = self.calculate_total_points()\n",
    "            reward += total_points\n",
    "        \n",
    "        info = {}\n",
    "        info.update({\n",
    "            'round': self.current_round,\n",
    "            'pick': self.pick,\n",
    "            'adp_adj': self.adp_adj,\n",
    "            'selected_position': selected_position,\n",
    "            'selected_player': selected_player,\n",
    "            'selected_player_points': selected_player_points,\n",
    "            'reward': reward,\n",
    "            \n",
    "        })\n",
    "        \n",
    "        self.current_round += 1\n",
    "                \n",
    "        # https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/#sphx-glr-tutorials-gymnasium-basics-environment-creation-py\n",
    "        return observation, reward, done, False, info\n",
    "    \n",
    "    def create_player_df(self):\n",
    "\n",
    "        player_adp_df = pd.read_csv(f'adp_{self.data_first_year}_{self.data_last_year}.csv')\n",
    "        player_adp_df = player_adp_df[player_adp_df['Player'].notna()]\n",
    "        player_adp_df = player_adp_df[player_adp_df['Year'] == self.year]\n",
    "        player_adp_df['Player'] = player_adp_df['Player'].apply(lambda x: ' '.join(str(x).split()[:2]))\n",
    "        \n",
    "        player_performance_df = pd.read_csv(f'player_performance_{self.data_first_year}_{self.data_last_year}.csv')\n",
    "        player_performance_df = player_performance_df[player_performance_df['Player'].notna()]\n",
    "        player_performance_df = player_performance_df[player_performance_df['year'] == self.year]\n",
    "        player_performance_df['Player'] = player_performance_df['Player'].apply(lambda x: ' '.join(str(x).split()[:2]))\n",
    "        \n",
    "        player_df = pd.merge(player_adp_df, player_performance_df, on='Player', how='inner')\n",
    "        player_df['Position'] = player_df['Position'].apply(lambda x: x[:2])        \n",
    "        player_df['Position'] = player_df['Position'].str[:2]        \n",
    "        player_df= player_df[['Player', 'Position', 'AVG', 'FPTS']]\n",
    "        \n",
    "        return player_df\n",
    "    \n",
    "    def snake_draft_pick(self, teams, round, first_round_pick):\n",
    "        if round % 2 == 1:\n",
    "            return (round - 1) * teams + first_round_pick\n",
    "        else:\n",
    "            return round * teams - first_round_pick + 1\n",
    "\n",
    "    \n",
    "    def calc_adp_adj(self):\n",
    "        if self.current_round == 1:\n",
    "            adp_adj = random.randint(int(-(self.first_round_pick - 1)/2), int((self.first_round_pick - 1)/2))\n",
    "        elif self.current_round == 2:\n",
    "            adp_adj = random.randint(int(-self.teams/2), int(self.teams/2))\n",
    "        else:\n",
    "            adp_adj = random.randint(-self.teams, self.teams)\n",
    "        return adp_adj\n",
    "    \n",
    "    \n",
    "    def draft_player(self, selected_position):\n",
    "        \n",
    "        adp_adj = self.calc_adp_adj()\n",
    "        \n",
    "        available_players = self.player_df[(self.player_df['Position'] == selected_position) & (self.player_df['AVG'] >= (self.pick + adp_adj)) & (~self.player_df['Player'].isin(self.drafted_players))]  # Filter out players who have already been drafted\n",
    "\n",
    "        selected_player_df = available_players.nsmallest(1, 'AVG')\n",
    "        selected_player = selected_player_df['Player'].iloc[0]\n",
    "        selected_player_points = selected_player_df['FPTS'].iloc[0]\n",
    "        \n",
    "        self.drafted_players.append(selected_player)\n",
    "        self.adp_adj = adp_adj\n",
    "        \n",
    "        return selected_player, selected_player_points\n",
    "    \n",
    "    def custom_policy(self):\n",
    "        \n",
    "        eligible_pos = []\n",
    "        for pos in self.roster.keys():\n",
    "            if len(self.roster[pos]) < self.position_counts[pos]:\n",
    "                eligible_pos.append(pos)\n",
    "        if self.flex_count < self.position_counts['FLEX']:\n",
    "            eligible_pos += self.flex_positions\n",
    "        \n",
    "        adp_adj = self.calc_adp_adj()\n",
    "        \n",
    "        available_players = self.player_df[(self.player_df['Position'].isin(eligible_pos)) & (self.player_df['AVG'] >= (self.pick + adp_adj)) & (~self.player_df['Player'].isin(self.drafted_players))]  # Filter out players who have already been drafted\n",
    "\n",
    "        selected_player_df = available_players.nsmallest(1, 'AVG')\n",
    "        selected_player = selected_player_df['Player'].iloc[0]\n",
    "        selected_player_position = selected_player_df['Position'].iloc[0]\n",
    "        selected_player_points = selected_player_df['FPTS'].iloc[0]\n",
    "        \n",
    "        key_found = None\n",
    "\n",
    "        for key, value in position_map.items():\n",
    "            if value == selected_player_position:\n",
    "                action = key\n",
    "                break\n",
    "                    \n",
    "        return action\n",
    "        \n",
    "    \n",
    "    def calculate_total_points(self):\n",
    "        \n",
    "        # note this method assumes you play your best player not the order they are drafted\n",
    "        \n",
    "        total_points = 0\n",
    "        \n",
    "        for pos in self.roster.keys():\n",
    "            pos_points_list = [self.player_df[self.player_df['Player'] == player]['FPTS'].iloc[0] for player in self.roster[pos]]\n",
    "            pos_points_list = sorted(pos_points_list, reverse=True)[:self.position_counts[pos]]\n",
    "            total_points += sum(pos_points_list)\n",
    "            \n",
    "        flex_points_list = []\n",
    "        for pos in self.flex_positions:\n",
    "            pos_points_list = [self.player_df[self.player_df['Player'] == player]['FPTS'].iloc[0] for player in self.roster[pos]]\n",
    "            pos_points_list = sorted(pos_points_list, reverse=True)[self.position_counts[pos]:]\n",
    "            flex_points_list+=pos_points_list        \n",
    "        flex_points_list = sorted(flex_points_list, reverse=True)[:self.position_counts[pos]]\n",
    "        \n",
    "        total_points+=sum(flex_points_list)\n",
    "        \n",
    "        return total_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_job(model_type\n",
    "                    , use_wandb = 'y', wandb_verbose=2\n",
    "                    , timesteps=1_000_000\n",
    "                    # , policy='MultiInputPolicy'\n",
    "                    , policy='MlpPolicy'\n",
    "                    # should look into mandating that each pick position is considered\n",
    "                    , n_eval_episodes=12\n",
    "                    , vec_envs='n', n_envs=4\n",
    "                    , sb3_model_verbose=0\n",
    "                    # DQN\n",
    "                    , dqn_exploration_final_eps=0.025, dqn_exploration_fraction=0.5\n",
    "                    # PPO\n",
    "                    # https://colab.research.google.com/drive/1GI0WpThwRHbl-Fu2RHfczq6dci5GBDVE#scrollTo=FMdJRrZ4n7xp\n",
    "                    , ppo_n_steps = 1024, ppo_batch_size = 64, ppo_n_epochs = 4, ppo_gamma = 0.999, ppo_gae_lambda = 0.98, ppo_ent_coef = 0.01\n",
    "                    , name_suffix = ''\n",
    "                    ):\n",
    "    \n",
    "    config = {\n",
    "    \"policy_type\": policy,\n",
    "    \"total_timesteps\": timesteps,\n",
    "    # \"env_id\": \"NflEnv\",\n",
    "    \"env_id\": \"FantasyFootballEnv\",\n",
    "    }\n",
    "\n",
    "    # https://stable-baselines3.readthedocs.io/en/master/guide/integrations.html\n",
    "    if use_wandb == 'y':\n",
    "        run = wandb.init(\n",
    "            # project=\"sb3_nfl_2\",\n",
    "            project=\"sb3_FantasyFootballEnv\",\n",
    "            config=config,\n",
    "            sync_tensorboard=True\n",
    "        )\n",
    "\n",
    "    # when using multiple environments, the total number of steps taken in counts each step taken in each environment\n",
    "    # if using 4 environments and 400_000 TIMESTEPS, the agent will take a total of 100_000 steps in each environment.\n",
    "    if vec_envs == 'y':\n",
    "        # https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/5_custom_gym_env.ipynb\n",
    "        # received 'ValueError: high is out of bounds for int32' without the seed\n",
    "        # env = make_vec_env(env_id = NflEnv, n_envs=n_envs, seed=1)\n",
    "        # eval_env = make_vec_env(env_id = NflEnv, n_envs=1, seed=1)\n",
    "        env = make_vec_env(env_id = FantasyFootballEnv, n_envs=n_envs, seed=1)\n",
    "        eval_env = make_vec_env(env_id = FantasyFootballEnv, n_envs=1, seed=1)\n",
    "\n",
    "    elif vec_envs == 'n':\n",
    "        # env = NflEnv()\n",
    "        # eval_env = NflEnv()\n",
    "        env = FantasyFootballEnv()\n",
    "        eval_env = FantasyFootballEnv()\n",
    "    \n",
    "    if use_wandb == 'y':\n",
    "        if model_type == 'DQN':\n",
    "            # default values for these parameters are exploration_final_eps=0.05 and exploration_fraction=0.1\n",
    "            # with the default values, the exploration rate will linearly decrease to 0.05 over the first 10% of the timesteps\n",
    "            model = DQN(config[\"policy_type\"], env, verbose=sb3_model_verbose, tensorboard_log=f\"runs/{run.id}\"\n",
    "                        , exploration_final_eps=dqn_exploration_final_eps, exploration_fraction = dqn_exploration_fraction)\n",
    "        elif model_type == 'PPO':\n",
    "            model = PPO(config[\"policy_type\"], env, verbose=sb3_model_verbose, tensorboard_log=f\"runs/{run.id}\"\n",
    "                        , n_steps = ppo_n_steps, batch_size = ppo_batch_size, n_epochs = ppo_n_epochs, gamma = ppo_gamma\n",
    "                        , gae_lambda = ppo_gae_lambda, ent_coef = ppo_ent_coef)\n",
    "        elif model_type == 'A2C':\n",
    "            model = A2C(config[\"policy_type\"], env, verbose=sb3_model_verbose, tensorboard_log=f\"runs/{run.id}\")\n",
    "    else:\n",
    "        if model_type == 'DQN':\n",
    "            model = DQN(config[\"policy_type\"], env, verbose=sb3_model_verbose, exploration_final_eps=dqn_exploration_final_eps\n",
    "                        , exploration_fraction = dqn_exploration_fraction)\n",
    "        elif model_type == 'PPO':\n",
    "            model = PPO(config[\"policy_type\"], env, verbose=sb3_model_verbose\n",
    "                        , n_steps = ppo_n_steps, batch_size = ppo_batch_size, n_epochs = ppo_n_epochs, gamma = ppo_gamma\n",
    "                        , gae_lambda = ppo_gae_lambda, ent_coef = ppo_ent_coef)\n",
    "        elif model_type == 'A2C':\n",
    "            model = A2C(config[\"policy_type\"], env, verbose=sb3_model_verbose)\n",
    "\n",
    "    mean_reward, std_reward = evaluate_policy(model=model, env=eval_env, n_eval_episodes=n_eval_episodes)\n",
    "    print(f\"mean_reward before training:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "    if use_wandb == 'y':\n",
    "        model.learn(\n",
    "            total_timesteps=config[\"total_timesteps\"],\n",
    "            callback=WandbCallback(\n",
    "                model_save_path=f\"models/{run.id}\",\n",
    "                verbose=wandb_verbose,\n",
    "            ),\n",
    "        )\n",
    "        run.finish()\n",
    "    else:\n",
    "        model.learn(total_timesteps=config[\"total_timesteps\"])\n",
    "\n",
    "    mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=n_eval_episodes)\n",
    "    print(f\"mean_reward after training:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "    # parameters_saved = model.get_parameters()\n",
    "    \n",
    "    if vec_envs == 'y':\n",
    "        model.save(f\"models/{model_type}_{timesteps}_vecEnv{name_suffix}\")\n",
    "    else:\n",
    "        model.save(f\"models/{model_type}_{timesteps}{name_suffix}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\grant\\anaconda3\\envs\\rl\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward before training:908.66 +/- 63.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\grant\\anaconda3\\envs\\rl\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward after training:1274.97 +/- 87.12\n"
     ]
    }
   ],
   "source": [
    "# model_type = 'PPO'\n",
    "# time_steps = 250_000\n",
    "# model = run_training_job(model_type,timesteps=time_steps, use_wandb='n', ppo_gamma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_type = 'PPO'\n",
    "# time_steps = 500_000\n",
    "# model = run_training_job(model_type,timesteps=time_steps, use_wandb='n', ppo_gamma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draft(teams=12, rounds=7, first_round_pick=None, year=None, model=None, actions=None, custom_policy=False):\n",
    "    env = FantasyFootballEnv(teams=teams, rounds=rounds, first_round_pick=first_round_pick, year=year)\n",
    "    state, info = env.reset(first_round_pick=first_round_pick, year=year)\n",
    "    for i in range(rounds):\n",
    "        if model is not None:\n",
    "            action, _states = model.predict(state)\n",
    "            action = int(action)  # If action is an array with a single value that can be directly converted to int\n",
    "        elif actions is not None:\n",
    "            action = actions[i]\n",
    "        \n",
    "        elif custom_policy:\n",
    "            action = env.custom_policy()\n",
    "        \n",
    "        new_state, reward, done, placeholder, info = env.step(action)\n",
    "        print(f'Round {info[\"round\"]}, Pick {info[\"pick\"]}, adp_adj {info[\"adp_adj\"]}, Pos {info[\"selected_position\"]}, Player {info[\"selected_player\"]}, points {info[\"selected_player_points\"]}, reward {info[\"reward\"]}')\n",
    "        state = new_state\n",
    "        if done:\n",
    "            print(f'total score: {reward}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first round pick: 1\n",
      "\n",
      "run first manual\n",
      "Round 1, Pick 1, adp_adj 0, Pos RB, Player Jonathan Taylor, points 132.4, reward 0\n",
      "Round 2, Pick 24, adp_adj -6, Pos RB, Player Leonard Fournette, points 190.6, reward 0\n",
      "Round 3, Pick 25, adp_adj -1, Pos WR, Player Michael Pittman, points 167.0, reward 0\n",
      "Round 4, Pick 48, adp_adj -7, Pos WR, Player Terry McLaurin, points 190.5, reward 0\n",
      "Round 5, Pick 49, adp_adj 10, Pos WR, Player Brandin Cooks, points 117.1, reward 0\n",
      "Round 6, Pick 72, adp_adj 5, Pos TE, Player Dawson Knox, points 111.7, reward 0\n",
      "Round 7, Pick 73, adp_adj -4, Pos QB, Player Tom Brady, points 280.5, reward 1189.8\n",
      "total score: 1189.8\n",
      "\n",
      "custom_policy\n",
      "Round 1, Pick 1, adp_adj 0, Pos RB, Player Jonathan Taylor, points 132.4, reward 0\n",
      "Round 2, Pick 24, adp_adj 5, Pos RB, Player James Conner, points 177.2, reward 0\n",
      "Round 3, Pick 25, adp_adj 2, Pos QB, Player Patrick Mahomes, points 428.9, reward 0\n",
      "Round 4, Pick 48, adp_adj 11, Pos WR, Player Brandin Cooks, points 117.1, reward 0\n",
      "Round 5, Pick 49, adp_adj -6, Pos WR, Player Courtland Sutton, points 127.4, reward 0\n",
      "Round 6, Pick 72, adp_adj 12, Pos TE, Player Zach Ertz, points 92.1, reward 0\n",
      "Round 7, Pick 73, adp_adj -7, Pos RB, Player Clyde Edwards-Helaire, points 89.8, reward 1164.8999999999999\n",
      "total score: 1164.8999999999999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "year = 2022\n",
    "first_round_pick=1\n",
    "print(f'first round pick: {first_round_pick}\\n')\n",
    "\n",
    "print('run first manual')\n",
    "# rb first\n",
    "actions = [1, 1, 2, 2, 2, 3, 0]\n",
    "draft(first_round_pick=first_round_pick, year=year, actions=actions)\n",
    "\n",
    "print('custom_policy')\n",
    "draft(first_round_pick=first_round_pick, year=year, custom_policy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year: 2020, first round pick: 8\n",
      "\n",
      "agent 1\n",
      "Round 1, Pick 8, adp_adj -3, Pos WR, Player Michael Thomas, points 63.9, reward 0\n",
      "Round 2, Pick 17, adp_adj 3, Pos TE, Player George Kittle, points 101.1, reward 0\n",
      "Round 3, Pick 32, adp_adj 8, Pos QB, Player Dak Prescott, points 139.1, reward 0\n",
      "Round 4, Pick 41, adp_adj 0, Pos RB, Player Le'Veon Bell, points 66.6, reward 0\n",
      "Round 5, Pick 56, adp_adj 5, Pos RB, Player D'Andre Swift, points 166.8, reward 0\n",
      "Round 6, Pick 65, adp_adj 9, Pos RB, Player Ronald Jones, points 172.3, reward 0\n",
      "Round 7, Pick 80, adp_adj 4, Pos WR, Player Julian Edelman, points 45.7, reward 755.5000000000001\n",
      "total score: 755.5000000000001\n",
      "\n",
      "agent 2\n",
      "Round 1, Pick 8, adp_adj 1, Pos QB, Player Patrick Mahomes, points 380.3, reward 0\n",
      "Round 2, Pick 17, adp_adj 6, Pos TE, Player Mark Andrews, points 141.1, reward 0\n",
      "Round 3, Pick 32, adp_adj 5, Pos RB, Player Jonathan Taylor, points 234.8, reward 0\n",
      "Round 4, Pick 41, adp_adj -6, Pos WR, Player A.J. Brown, points 212.5, reward 0\n",
      "Round 5, Pick 56, adp_adj -11, Pos WR, Player Tyler Lockett, points 215.4, reward 0\n",
      "Round 6, Pick 65, adp_adj -3, Pos RB, Player D'Andre Swift, points 166.8, reward 0\n",
      "Round 7, Pick 80, adp_adj 6, Pos WR, Player Deebo Samuel, points 64.2, reward 1415.1000000000001\n",
      "total score: 1415.1000000000001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "year = random.randint(2018, 2022)\n",
    "first_round_pick = random.randint(1, 12)\n",
    "model_type = 'PPO'\n",
    "\n",
    "print(f'year: {year}, first round pick: {first_round_pick}\\n')\n",
    "\n",
    "print('agent 1')\n",
    "time_steps = 250_000\n",
    "model = PPO.load(f'models/{model_type}_{time_steps}')\n",
    "draft(first_round_pick=first_round_pick, year=year, model=model)\n",
    "print('agent 2')\n",
    "time_steps = 500_000\n",
    "model = PPO.load(f'models/{model_type}_{time_steps}')\n",
    "draft(first_round_pick=first_round_pick, year=year, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year: 2020, first round pick: 9\n",
      "\n",
      "custom_policy\n",
      "Round 1, Pick 9, adp_adj 4, Pos WR, Player Julio Jones, points 120.6, reward 0\n",
      "Round 2, Pick 16, adp_adj 0, Pos RB, Player Austin Ekeler, points 138.3, reward 0\n",
      "Round 3, Pick 33, adp_adj -3, Pos WR, Player Odell Beckham, points 75.3, reward 0\n",
      "Round 4, Pick 40, adp_adj -3, Pos WR, Player A.J. Brown, points 212.5, reward 0\n",
      "Round 5, Pick 57, adp_adj -8, Pos RB, Player Raheem Mostert, points 91.7, reward 0\n",
      "Round 6, Pick 64, adp_adj -1, Pos QB, Player Matt Ryan, points 293.3, reward 0\n",
      "Round 7, Pick 81, adp_adj -1, Pos TE, Player Rob Gronkowski, points 126.8, reward 1058.5\n",
      "total score: 1058.5\n",
      "\n",
      "agent\n",
      "Round 1, Pick 9, adp_adj -2, Pos QB, Player Patrick Mahomes, points 380.3, reward 0\n",
      "Round 2, Pick 16, adp_adj 2, Pos TE, Player Travis Kelce, points 260.3, reward 0\n",
      "Round 3, Pick 33, adp_adj -3, Pos RB, Player Chris Carson, points 169.3, reward 0\n",
      "Round 4, Pick 40, adp_adj 8, Pos WR, Player Tyler Lockett, points 215.4, reward 0\n",
      "Round 5, Pick 57, adp_adj -12, Pos WR, Player Courtland Sutton, points 8.1, reward 0\n",
      "Round 6, Pick 64, adp_adj -1, Pos RB, Player D'Andre Swift, points 166.8, reward 0\n",
      "Round 7, Pick 81, adp_adj -9, Pos WR, Player Marquise Brown, points 154.0, reward 1354.2\n",
      "total score: 1354.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "year = random.randint(2018, 2022)\n",
    "first_round_pick = random.randint(1, 12)\n",
    "model_type = 'PPO'\n",
    "\n",
    "print(f'year: {year}, first round pick: {first_round_pick}\\n')\n",
    "\n",
    "print('custom_policy')\n",
    "draft(first_round_pick=first_round_pick, year=year, custom_policy=True)\n",
    "\n",
    "print('agent')\n",
    "time_steps = 500_000\n",
    "model = PPO.load(f'models/{model_type}_{time_steps}')\n",
    "draft(first_round_pick=first_round_pick, year=year, model=model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
