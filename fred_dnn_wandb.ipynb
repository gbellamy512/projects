{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "\n",
    "# set seed for reproducibility\n",
    "random_seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature count: 25\n",
      "row count: 236\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GDP</th>\n",
       "      <th>GDPC1</th>\n",
       "      <th>GDPPOT</th>\n",
       "      <th>CPIAUCSL</th>\n",
       "      <th>CPILFESL</th>\n",
       "      <th>GDPDEF</th>\n",
       "      <th>M1V</th>\n",
       "      <th>M2V</th>\n",
       "      <th>DFF</th>\n",
       "      <th>UNRATE</th>\n",
       "      <th>...</th>\n",
       "      <th>MANEMP</th>\n",
       "      <th>DSPIC96</th>\n",
       "      <th>PCE</th>\n",
       "      <th>PCEDG</th>\n",
       "      <th>PSAVERT</th>\n",
       "      <th>DSPI</th>\n",
       "      <th>INDPRO</th>\n",
       "      <th>HOUST</th>\n",
       "      <th>GPDI</th>\n",
       "      <th>MSPUS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1963-01-01</th>\n",
       "      <td>621.672</td>\n",
       "      <td>3628.306</td>\n",
       "      <td>3662.738125</td>\n",
       "      <td>30.44</td>\n",
       "      <td>31.5</td>\n",
       "      <td>17.134</td>\n",
       "      <td>4.178</td>\n",
       "      <td>1.690</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.7</td>\n",
       "      <td>...</td>\n",
       "      <td>15545.0</td>\n",
       "      <td>2541.1</td>\n",
       "      <td>374.4</td>\n",
       "      <td>53.1</td>\n",
       "      <td>10.9</td>\n",
       "      <td>430.0</td>\n",
       "      <td>26.0448</td>\n",
       "      <td>1244.0</td>\n",
       "      <td>99.689</td>\n",
       "      <td>17800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963-04-01</th>\n",
       "      <td>629.752</td>\n",
       "      <td>3669.020</td>\n",
       "      <td>3701.698767</td>\n",
       "      <td>30.48</td>\n",
       "      <td>31.7</td>\n",
       "      <td>17.164</td>\n",
       "      <td>4.194</td>\n",
       "      <td>1.675</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.7</td>\n",
       "      <td>...</td>\n",
       "      <td>15602.0</td>\n",
       "      <td>2547.1</td>\n",
       "      <td>376.4</td>\n",
       "      <td>53.2</td>\n",
       "      <td>10.7</td>\n",
       "      <td>431.1</td>\n",
       "      <td>26.7473</td>\n",
       "      <td>1689.0</td>\n",
       "      <td>101.650</td>\n",
       "      <td>18000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963-07-01</th>\n",
       "      <td>644.444</td>\n",
       "      <td>3749.681</td>\n",
       "      <td>3741.388301</td>\n",
       "      <td>30.69</td>\n",
       "      <td>31.8</td>\n",
       "      <td>17.187</td>\n",
       "      <td>4.248</td>\n",
       "      <td>1.680</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.6</td>\n",
       "      <td>...</td>\n",
       "      <td>15646.0</td>\n",
       "      <td>2572.6</td>\n",
       "      <td>384.4</td>\n",
       "      <td>55.5</td>\n",
       "      <td>10.1</td>\n",
       "      <td>438.0</td>\n",
       "      <td>27.0445</td>\n",
       "      <td>1614.0</td>\n",
       "      <td>104.612</td>\n",
       "      <td>17900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963-10-01</th>\n",
       "      <td>653.938</td>\n",
       "      <td>3774.264</td>\n",
       "      <td>3781.880559</td>\n",
       "      <td>30.75</td>\n",
       "      <td>32.0</td>\n",
       "      <td>17.326</td>\n",
       "      <td>4.269</td>\n",
       "      <td>1.672</td>\n",
       "      <td>3.50</td>\n",
       "      <td>5.5</td>\n",
       "      <td>...</td>\n",
       "      <td>15714.0</td>\n",
       "      <td>2617.3</td>\n",
       "      <td>386.0</td>\n",
       "      <td>54.2</td>\n",
       "      <td>11.5</td>\n",
       "      <td>447.0</td>\n",
       "      <td>27.5578</td>\n",
       "      <td>1779.0</td>\n",
       "      <td>107.189</td>\n",
       "      <td>18500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964-01-01</th>\n",
       "      <td>669.822</td>\n",
       "      <td>3853.835</td>\n",
       "      <td>3822.450115</td>\n",
       "      <td>30.94</td>\n",
       "      <td>32.2</td>\n",
       "      <td>17.381</td>\n",
       "      <td>4.345</td>\n",
       "      <td>1.685</td>\n",
       "      <td>3.25</td>\n",
       "      <td>5.6</td>\n",
       "      <td>...</td>\n",
       "      <td>15715.0</td>\n",
       "      <td>2652.8</td>\n",
       "      <td>396.8</td>\n",
       "      <td>57.9</td>\n",
       "      <td>10.7</td>\n",
       "      <td>455.3</td>\n",
       "      <td>27.8820</td>\n",
       "      <td>1603.0</td>\n",
       "      <td>110.474</td>\n",
       "      <td>18500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                GDP     GDPC1       GDPPOT  CPIAUCSL  CPILFESL  GDPDEF    M1V  \\\n",
       "Date                                                                            \n",
       "1963-01-01  621.672  3628.306  3662.738125     30.44      31.5  17.134  4.178   \n",
       "1963-04-01  629.752  3669.020  3701.698767     30.48      31.7  17.164  4.194   \n",
       "1963-07-01  644.444  3749.681  3741.388301     30.69      31.8  17.187  4.248   \n",
       "1963-10-01  653.938  3774.264  3781.880559     30.75      32.0  17.326  4.269   \n",
       "1964-01-01  669.822  3853.835  3822.450115     30.94      32.2  17.381  4.345   \n",
       "\n",
       "              M2V   DFF  UNRATE  ...   MANEMP  DSPIC96    PCE  PCEDG  PSAVERT  \\\n",
       "Date                             ...                                            \n",
       "1963-01-01  1.690  3.00     5.7  ...  15545.0   2541.1  374.4   53.1     10.9   \n",
       "1963-04-01  1.675  3.00     5.7  ...  15602.0   2547.1  376.4   53.2     10.7   \n",
       "1963-07-01  1.680  3.00     5.6  ...  15646.0   2572.6  384.4   55.5     10.1   \n",
       "1963-10-01  1.672  3.50     5.5  ...  15714.0   2617.3  386.0   54.2     11.5   \n",
       "1964-01-01  1.685  3.25     5.6  ...  15715.0   2652.8  396.8   57.9     10.7   \n",
       "\n",
       "             DSPI   INDPRO   HOUST     GPDI    MSPUS  \n",
       "Date                                                  \n",
       "1963-01-01  430.0  26.0448  1244.0   99.689  17800.0  \n",
       "1963-04-01  431.1  26.7473  1689.0  101.650  18000.0  \n",
       "1963-07-01  438.0  27.0445  1614.0  104.612  17900.0  \n",
       "1963-10-01  447.0  27.5578  1779.0  107.189  18500.0  \n",
       "1964-01-01  455.3  27.8820  1603.0  110.474  18500.0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('fred_230718.csv', index_col='Date', parse_dates=True)\n",
    "df = df.asfreq('QS')\n",
    "earliest_date = '1963-01-01'\n",
    "latest_date = '2021-10-01'\n",
    "# # filter df index to be between earliest_date and latest_date\n",
    "df = df.loc[(df.index >= earliest_date) & (df.index <= latest_date)]\n",
    "df.dropna(axis=1, inplace=True)\n",
    "print(f'feature count: {len(df.columns) - 1}')\n",
    "print(f'row count: {len(df)}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set target and create, train, validate, and test datasets and then scale and transform them so they will work better with the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'MSPUS'\n",
    "\n",
    "drop_cols = ['GDPC1', 'GDPPOT', 'CPIAUCSL', 'CPILFESL', 'GDPDEF'\n",
    "            , 'M2V'\n",
    "            , 'DSPIC96', 'PCE', 'PCEDG', 'DSPI', 'INDPRO', 'GPDI']\n",
    "df.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "y = df[target]\n",
    "X = df.drop(columns=[target]).shift(1).dropna()\n",
    "y = y.loc[X.index] # Make sure y and X have the same rows after dropna\n",
    "\n",
    "# https://datascience.stackexchange.com/questions/15135/train-test-validation-set-splitting-in-sklearn\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=random_seed)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.25, random_state=random_seed)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)  # validation data should also be scaled\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Log-transform the target variable\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_valid_log = np.log1p(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "\n",
    "def train_model(X_train, y_train, \n",
    "                X_valid, y_valid,\n",
    "                layer_sizes=[100, 100], \n",
    "                activation=\"relu\", \n",
    "                kernel_initializer=\"he_normal\", \n",
    "                loss='mse',\n",
    "                learning_rate=0.001, \n",
    "                epochs=100,\n",
    "                batch_norm=False,\n",
    "                l1_l2=False,\n",
    "                l1=.01,\n",
    "                l2=.01,\n",
    "                metrics=['mse'],\n",
    "                wandb = 'y'):\n",
    "\n",
    "    # Create a sequential model\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # Add batch normalization and dense layers according to the layer_sizes\n",
    "    for size in layer_sizes:\n",
    "        if batch_norm:\n",
    "            model.add(tf.keras.layers.BatchNormalization())\n",
    "        if l1_l2:\n",
    "            model.add(tf.keras.layers.Dense(size, activation=activation, kernel_initializer=kernel_initializer\n",
    "                                            , kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "        else:\n",
    "            model.add(tf.keras.layers.Dense(size, activation=activation, kernel_initializer=kernel_initializer))\n",
    "\n",
    "    # Add a final Dense layer with no activation\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "    # Create the optimizer with the custom learning rate\n",
    "    sgd = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=loss, optimizer=sgd, metrics=metrics)\n",
    "    \n",
    "    if wandb == 'y':\n",
    "        # Add WandbMetricsLogger to log metrics and WandbModelCheckpoint to log model checkpoints\n",
    "        wandb_callbacks = [\n",
    "            WandbMetricsLogger(),\n",
    "            # WandbModelCheckpoint(filepath=\"my_model_{epoch:02d}\"),\n",
    "            # WandbModelCheckpoint(filepath=\"my_model_best\", save_best_only=True, monitor='val_loss'),\n",
    "        ]\n",
    "\n",
    "        # Train the model using the scaled data\n",
    "        model.fit(X_train, y_train, epochs=epochs, validation_data=(X_valid, y_valid), callbacks=wandb_callbacks)\n",
    "    else:\n",
    "        # Train the model using the scaled data\n",
    "        model.fit(X_train, y_train, epochs=epochs, validation_data=(X_valid, y_valid))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: azovk6yv\n",
      "Sweep URL: https://wandb.ai/grantbell/fred_dnn_sweep/sweeps/azovk6yv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vcaadsj6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_norm: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_initializer: he_normal\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1: 0.1669997122667422\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1_l2: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl2: 0.319413580861995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0044572913522162435\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmetrics: mae\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\grant\\OneDrive\\Desktop\\vscode\\py_fin\\wandb\\run-20230721_110053-vcaadsj6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/grantbell/fred_dnn_sweep/runs/vcaadsj6' target=\"_blank\">rosy-sweep-1</a></strong> to <a href='https://wandb.ai/grantbell/fred_dnn_sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/grantbell/fred_dnn_sweep/sweeps/azovk6yv' target=\"_blank\">https://wandb.ai/grantbell/fred_dnn_sweep/sweeps/azovk6yv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/grantbell/fred_dnn_sweep' target=\"_blank\">https://wandb.ai/grantbell/fred_dnn_sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/grantbell/fred_dnn_sweep/sweeps/azovk6yv' target=\"_blank\">https://wandb.ai/grantbell/fred_dnn_sweep/sweeps/azovk6yv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/grantbell/fred_dnn_sweep/runs/vcaadsj6' target=\"_blank\">https://wandb.ai/grantbell/fred_dnn_sweep/runs/vcaadsj6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 1s 40ms/step - loss: 418.2718 - mae: 4.4651 - val_loss: 378.8135 - val_mae: 2.1933\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 370.7211 - mae: 1.8459 - val_loss: 359.7202 - val_mae: 1.9256\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 351.3892 - mae: 1.3141 - val_loss: 343.4414 - val_mae: 2.0155\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 334.4197 - mae: 1.1018 - val_loss: 326.7636 - val_mae: 1.8005\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 318.3742 - mae: 0.9496 - val_loss: 311.0551 - val_mae: 1.7157\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 303.0534 - mae: 0.8400 - val_loss: 296.2177 - val_mae: 1.6551\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 288.4676 - mae: 0.7681 - val_loss: 283.1754 - val_mae: 1.9674\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 274.8377 - mae: 0.8487 - val_loss: 268.3930 - val_mae: 1.6078\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 260.8990 - mae: 0.6293 - val_loss: 255.3480 - val_mae: 1.6288\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 248.0071 - mae: 0.6149 - val_loss: 242.6354 - val_mae: 1.5762\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 235.6720 - mae: 0.6124 - val_loss: 230.6604 - val_mae: 1.5579\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 223.7912 - mae: 0.5787 - val_loss: 218.7858 - val_mae: 1.5151\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 212.3555 - mae: 0.5569 - val_loss: 208.1355 - val_mae: 1.6347\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 201.4572 - mae: 0.5704 - val_loss: 197.5977 - val_mae: 1.5712\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 190.9781 - mae: 0.5301 - val_loss: 187.0695 - val_mae: 1.4664\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 180.9854 - mae: 0.5275 - val_loss: 177.3187 - val_mae: 1.4791\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 171.3725 - mae: 0.5136 - val_loss: 168.0734 - val_mae: 1.5413\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 162.2204 - mae: 0.5096 - val_loss: 159.1495 - val_mae: 1.5422\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 153.4739 - mae: 0.5176 - val_loss: 150.5311 - val_mae: 1.4950\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 145.1034 - mae: 0.4992 - val_loss: 142.4484 - val_mae: 1.4219\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 137.1772 - mae: 0.4983 - val_loss: 134.9639 - val_mae: 1.5163\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 129.5414 - mae: 0.4841 - val_loss: 127.4829 - val_mae: 1.4578\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 122.4120 - mae: 0.5369 - val_loss: 120.5482 - val_mae: 1.4921\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 115.4472 - mae: 0.4740 - val_loss: 113.8844 - val_mae: 1.4554\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 108.9783 - mae: 0.5092 - val_loss: 107.5369 - val_mae: 1.5198\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 102.7728 - mae: 0.5034 - val_loss: 101.6164 - val_mae: 1.5372\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 96.9188 - mae: 0.5317 - val_loss: 95.7884 - val_mae: 1.4913\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 91.3991 - mae: 0.5249 - val_loss: 90.2323 - val_mae: 1.4600\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 86.0049 - mae: 0.4626 - val_loss: 85.2646 - val_mae: 1.4750\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 80.9868 - mae: 0.4742 - val_loss: 80.3154 - val_mae: 1.4331\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 76.2360 - mae: 0.4460 - val_loss: 75.7756 - val_mae: 1.4590\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 71.8073 - mae: 0.4760 - val_loss: 71.4745 - val_mae: 1.3912\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 67.5418 - mae: 0.4303 - val_loss: 67.2048 - val_mae: 1.3381\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 63.5421 - mae: 0.4212 - val_loss: 63.4849 - val_mae: 1.3409\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 59.7992 - mae: 0.4219 - val_loss: 60.1227 - val_mae: 1.4280\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 56.2231 - mae: 0.4090 - val_loss: 56.6354 - val_mae: 1.4180\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 52.9118 - mae: 0.4225 - val_loss: 53.3525 - val_mae: 1.3510\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 49.7641 - mae: 0.3958 - val_loss: 50.4809 - val_mae: 1.3634\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 46.8614 - mae: 0.3879 - val_loss: 47.4996 - val_mae: 1.3441\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 44.0468 - mae: 0.3763 - val_loss: 44.6071 - val_mae: 1.2918\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 41.4554 - mae: 0.3643 - val_loss: 42.1996 - val_mae: 1.2877\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 39.0334 - mae: 0.3621 - val_loss: 39.8056 - val_mae: 1.3011\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 36.7292 - mae: 0.3457 - val_loss: 37.8410 - val_mae: 1.3790\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 34.5873 - mae: 0.3585 - val_loss: 35.5810 - val_mae: 1.2876\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 32.5880 - mae: 0.3164 - val_loss: 33.6033 - val_mae: 1.2925\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 30.7105 - mae: 0.3130 - val_loss: 31.6677 - val_mae: 1.2479\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 28.9633 - mae: 0.3090 - val_loss: 30.0027 - val_mae: 1.2562\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 27.3202 - mae: 0.3025 - val_loss: 28.4586 - val_mae: 1.2508\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 25.7937 - mae: 0.2886 - val_loss: 27.0903 - val_mae: 1.2594\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 24.3642 - mae: 0.2830 - val_loss: 25.6507 - val_mae: 1.2410\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 23.0301 - mae: 0.2778 - val_loss: 24.3066 - val_mae: 1.2321\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 21.7852 - mae: 0.2765 - val_loss: 23.1517 - val_mae: 1.2308\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 20.6165 - mae: 0.2705 - val_loss: 22.0177 - val_mae: 1.1983\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 19.5196 - mae: 0.2516 - val_loss: 20.9829 - val_mae: 1.1985\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 18.5005 - mae: 0.2528 - val_loss: 19.9274 - val_mae: 1.1752\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 17.5235 - mae: 0.2175 - val_loss: 18.9130 - val_mae: 1.1702\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 16.6128 - mae: 0.2187 - val_loss: 17.9717 - val_mae: 1.1606\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 15.7766 - mae: 0.2473 - val_loss: 17.2111 - val_mae: 1.1781\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 14.9390 - mae: 0.2289 - val_loss: 16.3051 - val_mae: 1.1431\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 14.1553 - mae: 0.2172 - val_loss: 15.5232 - val_mae: 1.1311\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 13.4037 - mae: 0.2093 - val_loss: 14.8470 - val_mae: 1.1417\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 12.6932 - mae: 0.2078 - val_loss: 14.0723 - val_mae: 1.1252\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 12.0139 - mae: 0.2182 - val_loss: 13.3287 - val_mae: 1.1013\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 11.3606 - mae: 0.2140 - val_loss: 12.7792 - val_mae: 1.1026\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 10.7449 - mae: 0.2110 - val_loss: 12.0240 - val_mae: 1.0780\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 10.1405 - mae: 0.2077 - val_loss: 11.5868 - val_mae: 1.1171\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 9.5731 - mae: 0.2134 - val_loss: 10.9366 - val_mae: 1.1016\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 9.0361 - mae: 0.2348 - val_loss: 10.3946 - val_mae: 1.0839\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 8.5165 - mae: 0.2121 - val_loss: 9.9019 - val_mae: 1.0908\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 8.0257 - mae: 0.2186 - val_loss: 9.2979 - val_mae: 1.0509\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 7.5615 - mae: 0.2195 - val_loss: 8.8500 - val_mae: 1.0515\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 7.1193 - mae: 0.2210 - val_loss: 8.3963 - val_mae: 1.0472\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 6.7021 - mae: 0.2193 - val_loss: 8.0398 - val_mae: 1.0642\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 6.3097 - mae: 0.2327 - val_loss: 7.5944 - val_mae: 1.0324\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 5.9305 - mae: 0.2188 - val_loss: 7.1905 - val_mae: 1.0311\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 5.5742 - mae: 0.2332 - val_loss: 6.8143 - val_mae: 1.0112\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 5.2404 - mae: 0.2202 - val_loss: 6.4341 - val_mae: 1.0008\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4.9238 - mae: 0.2374 - val_loss: 6.1510 - val_mae: 1.0095\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4.6207 - mae: 0.2340 - val_loss: 5.8232 - val_mae: 0.9929\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4.3368 - mae: 0.2332 - val_loss: 5.4756 - val_mae: 0.9681\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4.0691 - mae: 0.2342 - val_loss: 5.2357 - val_mae: 0.9745\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 3.8213 - mae: 0.2390 - val_loss: 4.9695 - val_mae: 0.9716\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3.5833 - mae: 0.2474 - val_loss: 4.6887 - val_mae: 0.9418\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.3672 - mae: 0.2414 - val_loss: 4.4630 - val_mae: 0.9400\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 3.1721 - mae: 0.2517 - val_loss: 4.3555 - val_mae: 0.9738\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.9821 - mae: 0.2591 - val_loss: 4.1357 - val_mae: 0.9619\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 2.8079 - mae: 0.2567 - val_loss: 3.9155 - val_mae: 0.9393\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.6470 - mae: 0.2596 - val_loss: 3.6951 - val_mae: 0.9156\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.5025 - mae: 0.2627 - val_loss: 3.6125 - val_mae: 0.9379\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.3709 - mae: 0.2736 - val_loss: 3.4444 - val_mae: 0.9223\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 2.2520 - mae: 0.2671 - val_loss: 3.3689 - val_mae: 0.9339\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 2.1488 - mae: 0.2753 - val_loss: 3.1887 - val_mae: 0.9010\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.0554 - mae: 0.2744 - val_loss: 3.1185 - val_mae: 0.9166\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.9660 - mae: 0.2822 - val_loss: 3.0341 - val_mae: 0.9165\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.8876 - mae: 0.2806 - val_loss: 2.9386 - val_mae: 0.9074\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.8138 - mae: 0.2806 - val_loss: 2.8754 - val_mae: 0.9122\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.7537 - mae: 0.2913 - val_loss: 2.8160 - val_mae: 0.9121\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.6977 - mae: 0.2870 - val_loss: 2.7413 - val_mae: 0.9047\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.6492 - mae: 0.2898 - val_loss: 2.6809 - val_mae: 0.8953\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.6064 - mae: 0.2842 - val_loss: 2.6393 - val_mae: 0.8957\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▇▆▆▅▅▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/mae</td><td>█▃▂▂▂▂▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_loss</td><td>█▇▆▆▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_mae</td><td>█▇▅▅▅▅▄▄▄▄▄▄▄▃▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/epoch</td><td>99</td></tr><tr><td>epoch/learning_rate</td><td>0.00446</td></tr><tr><td>epoch/loss</td><td>1.60642</td></tr><tr><td>epoch/mae</td><td>0.2842</td></tr><tr><td>epoch/val_loss</td><td>2.63928</td></tr><tr><td>epoch/val_mae</td><td>0.89567</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rosy-sweep-1</strong> at: <a href='https://wandb.ai/grantbell/fred_dnn_sweep/runs/vcaadsj6' target=\"_blank\">https://wandb.ai/grantbell/fred_dnn_sweep/runs/vcaadsj6</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230721_110053-vcaadsj6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes',  # can be grid, random, or bayes\n",
    "    'metric': {\n",
    "        'name': 'val_loss',\n",
    "        'goal': 'minimize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'min': 0.0001,\n",
    "            'max': 0.01\n",
    "        },\n",
    "        'activation': {\n",
    "            'values': ['relu']\n",
    "        }, \n",
    "        'kernel_initializer': {\n",
    "            'values': ['he_normal']\n",
    "        }, \n",
    "        'loss': {\n",
    "            'values': ['mse']\n",
    "        }, \n",
    "        # 'activation': {\n",
    "        #     'values': ['relu', 'tanh', 'sigmoid', 'elu', 'selu', 'softplus']  # more activation functions\n",
    "        # }, \n",
    "        # 'kernel_initializer': {\n",
    "        #     'values': ['he_normal', 'glorot_uniform', 'glorot_normal', 'lecun_normal']  # more initializers\n",
    "        # }, \n",
    "        # 'loss': {\n",
    "        #     'values': ['mse', 'mae', 'logcosh', 'huber']  # more loss functions\n",
    "        # },\n",
    "        'epoch': {\n",
    "            'values': [100]\n",
    "        }, \n",
    "        # 'epoch': {\n",
    "        #     'min': 10,\n",
    "        #     'max': 500\n",
    "        # },\n",
    "        'batch_norm': {\n",
    "            'values': [True, False]\n",
    "        }, \n",
    "        'l1_l2': {\n",
    "            'values': [True]\n",
    "        },        \n",
    "        'l1': {\n",
    "            'min': 0.0001,\n",
    "            'max': 0.5\n",
    "        },\n",
    "        'l2': {\n",
    "            'min': 0.0001,\n",
    "            'max': 0.5\n",
    "        },\n",
    "        'metrics': {\n",
    "            'values': ['mae']\n",
    "        }\n",
    "        # 'metrics': {\n",
    "        #     'values': ['mae', 'mse', 'mape', 'msle']  # adding more metrics\n",
    "        # }\n",
    "    }\n",
    "}\n",
    "\n",
    "def train(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        model = train_model(\n",
    "            X_train=X_train_scaled, y_train=y_train_log,\n",
    "            X_valid=X_test_scaled, y_valid=y_valid_log,\n",
    "            activation=config.activation, kernel_initializer=config.kernel_initializer, \n",
    "            loss=config.loss, learning_rate=config.learning_rate, \n",
    "            epochs=config.epoch, batch_norm=config.batch_norm, \n",
    "            l1_l2=config.l1_l2, l1=config.l1, l2=config.l2, \n",
    "            metrics=config.metrics)\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"fred_dnn_sweep\")\n",
    "wandb.agent(sweep_id, train, count=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the best epoch/val_mae is run 94\n",
    "https://wandb.ai/grantbell/fred_dnn_sweep/runs/qgc616q6?workspace=user-grantbell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 112.5749 - mse: 112.5749 - val_loss: 46.4741 - val_mse: 46.4741\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 27.5989 - mse: 27.5989 - val_loss: 15.6532 - val_mse: 15.6532\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 11.1649 - mse: 11.1649 - val_loss: 11.5061 - val_mse: 11.5061\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 7.6305 - mse: 7.6305 - val_loss: 9.3956 - val_mse: 9.3956\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 5.9149 - mse: 5.9149 - val_loss: 8.8249 - val_mse: 8.8249\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 4.8155 - mse: 4.8155 - val_loss: 8.2504 - val_mse: 8.2504\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 4.1224 - mse: 4.1224 - val_loss: 7.9052 - val_mse: 7.9052\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 3.6729 - mse: 3.6729 - val_loss: 7.6365 - val_mse: 7.6365\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 3.3078 - mse: 3.3078 - val_loss: 7.0908 - val_mse: 7.0908\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 3.0092 - mse: 3.0092 - val_loss: 6.6100 - val_mse: 6.6100\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.7552 - mse: 2.7552 - val_loss: 6.4203 - val_mse: 6.4203\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.5463 - mse: 2.5463 - val_loss: 6.2361 - val_mse: 6.2361\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 2.3653 - mse: 2.3653 - val_loss: 6.2114 - val_mse: 6.2114\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.2159 - mse: 2.2159 - val_loss: 6.2298 - val_mse: 6.2298\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 2.1085 - mse: 2.1085 - val_loss: 5.9725 - val_mse: 5.9725\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.9600 - mse: 1.9600 - val_loss: 5.7015 - val_mse: 5.7015\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.8386 - mse: 1.8386 - val_loss: 5.7854 - val_mse: 5.7854\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.7313 - mse: 1.7313 - val_loss: 5.3784 - val_mse: 5.3784\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.6533 - mse: 1.6533 - val_loss: 5.4940 - val_mse: 5.4940\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.5775 - mse: 1.5775 - val_loss: 5.3202 - val_mse: 5.3202\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.4861 - mse: 1.4861 - val_loss: 5.0804 - val_mse: 5.0804\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.4123 - mse: 1.4123 - val_loss: 4.9091 - val_mse: 4.9091\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.3251 - mse: 1.3251 - val_loss: 5.0886 - val_mse: 5.0886\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.2681 - mse: 1.2681 - val_loss: 4.9719 - val_mse: 4.9719\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.1918 - mse: 1.1918 - val_loss: 4.7097 - val_mse: 4.7097\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1504 - mse: 1.1504 - val_loss: 4.5258 - val_mse: 4.5258\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.0861 - mse: 1.0861 - val_loss: 4.7867 - val_mse: 4.7867\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.0369 - mse: 1.0369 - val_loss: 4.5325 - val_mse: 4.5325\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.9867 - mse: 0.9867 - val_loss: 4.5959 - val_mse: 4.5959\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.9555 - mse: 0.9555 - val_loss: 4.3085 - val_mse: 4.3085\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.9044 - mse: 0.9044 - val_loss: 4.3188 - val_mse: 4.3188\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.8631 - mse: 0.8631 - val_loss: 4.1814 - val_mse: 4.1814\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.8299 - mse: 0.8299 - val_loss: 4.2701 - val_mse: 4.2701\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.8076 - mse: 0.8076 - val_loss: 4.0639 - val_mse: 4.0639\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.7701 - mse: 0.7701 - val_loss: 4.0586 - val_mse: 4.0586\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.7564 - mse: 0.7564 - val_loss: 4.1795 - val_mse: 4.1795\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.7322 - mse: 0.7322 - val_loss: 4.1788 - val_mse: 4.1788\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6956 - mse: 0.6956 - val_loss: 3.9545 - val_mse: 3.9545\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6767 - mse: 0.6767 - val_loss: 3.8135 - val_mse: 3.8135\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6507 - mse: 0.6507 - val_loss: 3.8359 - val_mse: 3.8359\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6344 - mse: 0.6344 - val_loss: 3.7856 - val_mse: 3.7856\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.6067 - mse: 0.6067 - val_loss: 3.7940 - val_mse: 3.7940\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5966 - mse: 0.5966 - val_loss: 3.7458 - val_mse: 3.7458\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5805 - mse: 0.5805 - val_loss: 3.8757 - val_mse: 3.8757\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5633 - mse: 0.5633 - val_loss: 3.5931 - val_mse: 3.5931\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5350 - mse: 0.5350 - val_loss: 3.6069 - val_mse: 3.6069\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5293 - mse: 0.5293 - val_loss: 3.5176 - val_mse: 3.5176\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5082 - mse: 0.5082 - val_loss: 3.5141 - val_mse: 3.5141\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5012 - mse: 0.5012 - val_loss: 3.3883 - val_mse: 3.3883\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4877 - mse: 0.4877 - val_loss: 3.5251 - val_mse: 3.5251\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4739 - mse: 0.4739 - val_loss: 3.4653 - val_mse: 3.4653\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4638 - mse: 0.4638 - val_loss: 3.3987 - val_mse: 3.3987\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4539 - mse: 0.4539 - val_loss: 3.4124 - val_mse: 3.4124\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4427 - mse: 0.4427 - val_loss: 3.4165 - val_mse: 3.4165\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4368 - mse: 0.4368 - val_loss: 3.3663 - val_mse: 3.3663\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4223 - mse: 0.4223 - val_loss: 3.3901 - val_mse: 3.3901\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4162 - mse: 0.4162 - val_loss: 3.3362 - val_mse: 3.3362\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4104 - mse: 0.4104 - val_loss: 3.2345 - val_mse: 3.2345\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.4021 - mse: 0.4021 - val_loss: 3.3034 - val_mse: 3.3034\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4027 - mse: 0.4027 - val_loss: 3.2749 - val_mse: 3.2749\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3919 - mse: 0.3919 - val_loss: 3.1907 - val_mse: 3.1907\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3812 - mse: 0.3812 - val_loss: 3.1871 - val_mse: 3.1871\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3745 - mse: 0.3745 - val_loss: 3.2808 - val_mse: 3.2808\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3664 - mse: 0.3664 - val_loss: 3.2123 - val_mse: 3.2123\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3606 - mse: 0.3606 - val_loss: 3.0845 - val_mse: 3.0845\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3559 - mse: 0.3559 - val_loss: 3.2222 - val_mse: 3.2222\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3508 - mse: 0.3508 - val_loss: 3.0973 - val_mse: 3.0973\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3461 - mse: 0.3461 - val_loss: 3.2382 - val_mse: 3.2382\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3402 - mse: 0.3402 - val_loss: 3.1476 - val_mse: 3.1476\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3364 - mse: 0.3364 - val_loss: 3.1059 - val_mse: 3.1059\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3282 - mse: 0.3282 - val_loss: 3.1393 - val_mse: 3.1393\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3249 - mse: 0.3249 - val_loss: 3.0739 - val_mse: 3.0739\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3179 - mse: 0.3179 - val_loss: 3.0355 - val_mse: 3.0355\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3144 - mse: 0.3144 - val_loss: 3.0260 - val_mse: 3.0260\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3108 - mse: 0.3108 - val_loss: 3.0948 - val_mse: 3.0948\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.3055 - mse: 0.3055 - val_loss: 2.9791 - val_mse: 2.9791\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3016 - mse: 0.3016 - val_loss: 3.0081 - val_mse: 3.0081\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2971 - mse: 0.2971 - val_loss: 3.0671 - val_mse: 3.0671\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2940 - mse: 0.2940 - val_loss: 3.0074 - val_mse: 3.0074\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2918 - mse: 0.2918 - val_loss: 2.9334 - val_mse: 2.9334\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2867 - mse: 0.2867 - val_loss: 2.9753 - val_mse: 2.9753\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2836 - mse: 0.2836 - val_loss: 3.0291 - val_mse: 3.0291\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2792 - mse: 0.2792 - val_loss: 2.9571 - val_mse: 2.9571\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.2759 - mse: 0.2759 - val_loss: 2.9217 - val_mse: 2.9217\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2745 - mse: 0.2745 - val_loss: 2.9491 - val_mse: 2.9491\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.2698 - mse: 0.2698 - val_loss: 2.9695 - val_mse: 2.9695\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.2689 - mse: 0.2689 - val_loss: 2.9527 - val_mse: 2.9527\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2666 - mse: 0.2666 - val_loss: 2.9400 - val_mse: 2.9400\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2617 - mse: 0.2617 - val_loss: 2.8938 - val_mse: 2.8938\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2570 - mse: 0.2570 - val_loss: 2.9960 - val_mse: 2.9960\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.2614 - mse: 0.2614 - val_loss: 2.9170 - val_mse: 2.9170\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.2508 - mse: 0.2508 - val_loss: 2.9090 - val_mse: 2.9090\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2500 - mse: 0.2500 - val_loss: 2.9354 - val_mse: 2.9354\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2450 - mse: 0.2450 - val_loss: 2.8533 - val_mse: 2.8533\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2457 - mse: 0.2457 - val_loss: 2.8630 - val_mse: 2.8630\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2392 - mse: 0.2392 - val_loss: 2.8250 - val_mse: 2.8250\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2383 - mse: 0.2383 - val_loss: 2.8442 - val_mse: 2.8442\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2340 - mse: 0.2340 - val_loss: 2.8237 - val_mse: 2.8237\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.2330 - mse: 0.2330 - val_loss: 2.7540 - val_mse: 2.7540\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2323 - mse: 0.2323 - val_loss: 2.8202 - val_mse: 2.8202\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Test set MSE: 24376347000.0\n",
      "Test set RMSE: 156129.27\n",
      "Test set RMSPE (%): 68.96343363316426\n",
      "run 94\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 1s 46ms/step - loss: 569.4186 - mse: 41.9471 - val_loss: 523.6353 - val_mse: 7.7323\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 515.2791 - mse: 5.8402 - val_loss: 504.1984 - val_mse: 6.6481\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 495.4175 - mse: 4.1794 - val_loss: 485.6802 - val_mse: 6.1619\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 476.8819 - mse: 3.4023 - val_loss: 467.6409 - val_mse: 5.6382\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 458.8847 - mse: 2.7679 - val_loss: 450.4916 - val_mse: 5.3536\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 441.5875 - mse: 2.2142 - val_loss: 433.7348 - val_mse: 5.0644\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 424.9112 - mse: 1.8523 - val_loss: 418.0148 - val_mse: 5.4137\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 408.8040 - mse: 1.6249 - val_loss: 401.5330 - val_mse: 4.3226\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 393.2292 - mse: 1.3807 - val_loss: 386.6035 - val_mse: 4.6002\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 378.0809 - mse: 1.2053 - val_loss: 371.4056 - val_mse: 4.0996\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 363.5042 - mse: 1.1478 - val_loss: 357.6351 - val_mse: 4.5330\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 349.3709 - mse: 1.0705 - val_loss: 343.2634 - val_mse: 3.7633\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 335.6931 - mse: 0.9269 - val_loss: 329.9886 - val_mse: 3.8901\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 322.4396 - mse: 0.8784 - val_loss: 317.4992 - val_mse: 4.3774\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 309.6388 - mse: 0.8494 - val_loss: 304.2522 - val_mse: 3.6276\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 297.2302 - mse: 0.8061 - val_loss: 292.1631 - val_mse: 3.5018\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 285.2613 - mse: 0.7288 - val_loss: 280.5347 - val_mse: 3.6798\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 273.6377 - mse: 0.6885 - val_loss: 269.4422 - val_mse: 3.9185\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 262.4704 - mse: 0.7412 - val_loss: 257.9267 - val_mse: 3.1441\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 251.6375 - mse: 0.6236 - val_loss: 247.4826 - val_mse: 3.4235\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 241.1577 - mse: 0.6383 - val_loss: 237.2030 - val_mse: 3.2816\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 231.0545 - mse: 0.5807 - val_loss: 227.3604 - val_mse: 3.3279\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 221.3027 - mse: 0.6109 - val_loss: 217.7481 - val_mse: 3.2188\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 211.8868 - mse: 0.5962 - val_loss: 208.4862 - val_mse: 3.0655\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 202.8142 - mse: 0.5375 - val_loss: 199.6917 - val_mse: 3.2563\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 194.0879 - mse: 0.6004 - val_loss: 191.0293 - val_mse: 3.0204\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 185.6689 - mse: 0.5708 - val_loss: 183.0530 - val_mse: 3.3574\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 177.5736 - mse: 0.5925 - val_loss: 175.2081 - val_mse: 3.3921\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 169.7272 - mse: 0.5579 - val_loss: 167.2713 - val_mse: 2.9981\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 162.2108 - mse: 0.5108 - val_loss: 160.1934 - val_mse: 3.2693\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 155.0112 - mse: 0.5513 - val_loss: 153.1295 - val_mse: 3.1335\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 148.1062 - mse: 0.5257 - val_loss: 146.0912 - val_mse: 2.9407\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 141.3985 - mse: 0.5105 - val_loss: 139.7131 - val_mse: 3.0624\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 135.0320 - mse: 0.5116 - val_loss: 133.6312 - val_mse: 3.1709\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 128.9041 - mse: 0.4866 - val_loss: 127.7066 - val_mse: 3.1923\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 123.0705 - mse: 0.5094 - val_loss: 122.0678 - val_mse: 3.2152\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 117.4786 - mse: 0.4791 - val_loss: 116.4950 - val_mse: 3.0580\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 112.1143 - mse: 0.4668 - val_loss: 111.1292 - val_mse: 2.8544\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 107.0093 - mse: 0.4689 - val_loss: 106.0105 - val_mse: 2.5821\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 102.1220 - mse: 0.4268 - val_loss: 101.4477 - val_mse: 2.8185\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 97.4301 - mse: 0.3999 - val_loss: 96.7942 - val_mse: 2.7561\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 92.9696 - mse: 0.4329 - val_loss: 92.4405 - val_mse: 2.7323\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 88.6378 - mse: 0.3763 - val_loss: 88.4151 - val_mse: 2.9276\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 84.5554 - mse: 0.4011 - val_loss: 84.3857 - val_mse: 2.7465\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 80.6290 - mse: 0.3392 - val_loss: 80.5798 - val_mse: 2.8178\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 76.9027 - mse: 0.3743 - val_loss: 77.1478 - val_mse: 2.8077\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 73.3829 - mse: 0.3249 - val_loss: 73.5103 - val_mse: 2.7204\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 69.9903 - mse: 0.3347 - val_loss: 70.4015 - val_mse: 2.7693\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 66.7980 - mse: 0.3129 - val_loss: 67.1499 - val_mse: 2.6873\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 63.7301 - mse: 0.2993 - val_loss: 64.0796 - val_mse: 2.5758\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 60.8284 - mse: 0.3024 - val_loss: 61.3770 - val_mse: 2.6247\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 58.0774 - mse: 0.2890 - val_loss: 58.6340 - val_mse: 2.5309\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 55.4712 - mse: 0.2697 - val_loss: 56.1451 - val_mse: 2.6015\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 52.9977 - mse: 0.2677 - val_loss: 53.6753 - val_mse: 2.4925\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 50.6579 - mse: 0.2625 - val_loss: 51.4707 - val_mse: 2.5594\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 48.4267 - mse: 0.2479 - val_loss: 49.2403 - val_mse: 2.4551\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 46.3202 - mse: 0.2382 - val_loss: 47.2755 - val_mse: 2.5528\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 44.3080 - mse: 0.2387 - val_loss: 45.2889 - val_mse: 2.4731\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 42.3987 - mse: 0.2080 - val_loss: 43.4878 - val_mse: 2.5498\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 40.6053 - mse: 0.2366 - val_loss: 41.6920 - val_mse: 2.4126\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 38.8875 - mse: 0.2089 - val_loss: 39.8960 - val_mse: 2.2182\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 37.2568 - mse: 0.1828 - val_loss: 38.3859 - val_mse: 2.3283\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 35.7048 - mse: 0.1819 - val_loss: 36.8370 - val_mse: 2.2658\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 34.2461 - mse: 0.1708 - val_loss: 35.4681 - val_mse: 2.3120\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 32.8704 - mse: 0.1708 - val_loss: 34.1632 - val_mse: 2.3461\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 31.5580 - mse: 0.1730 - val_loss: 32.7620 - val_mse: 2.1950\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 30.3130 - mse: 0.1603 - val_loss: 31.5589 - val_mse: 2.1803\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 29.1185 - mse: 0.1534 - val_loss: 30.4743 - val_mse: 2.2174\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 27.9800 - mse: 0.1334 - val_loss: 29.3757 - val_mse: 2.2685\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 26.8850 - mse: 0.1374 - val_loss: 28.2819 - val_mse: 2.2290\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 25.8320 - mse: 0.1374 - val_loss: 27.2284 - val_mse: 2.1399\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 24.8167 - mse: 0.1153 - val_loss: 26.2248 - val_mse: 2.1776\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 23.8335 - mse: 0.1183 - val_loss: 25.2486 - val_mse: 2.1545\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 22.8868 - mse: 0.1156 - val_loss: 24.3392 - val_mse: 2.1913\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 21.9788 - mse: 0.1248 - val_loss: 23.4367 - val_mse: 2.0596\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 21.0963 - mse: 0.0923 - val_loss: 22.5257 - val_mse: 2.0988\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 20.2395 - mse: 0.0978 - val_loss: 21.6347 - val_mse: 2.0172\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 19.4106 - mse: 0.0936 - val_loss: 20.8343 - val_mse: 2.0225\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 18.6123 - mse: 0.0840 - val_loss: 20.0257 - val_mse: 2.0048\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 17.8310 - mse: 0.0803 - val_loss: 19.2386 - val_mse: 1.9836\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 17.0815 - mse: 0.0783 - val_loss: 18.4938 - val_mse: 1.9725\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 16.3568 - mse: 0.0780 - val_loss: 17.7635 - val_mse: 1.9434\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 15.6571 - mse: 0.0693 - val_loss: 17.0844 - val_mse: 1.9451\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 14.9812 - mse: 0.0721 - val_loss: 16.3564 - val_mse: 1.8775\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 14.3246 - mse: 0.0663 - val_loss: 15.7121 - val_mse: 1.8535\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 13.6943 - mse: 0.0616 - val_loss: 15.0693 - val_mse: 1.8425\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 13.0873 - mse: 0.0635 - val_loss: 14.4733 - val_mse: 1.8256\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 12.5031 - mse: 0.0612 - val_loss: 13.8589 - val_mse: 1.7846\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 11.9356 - mse: 0.0557 - val_loss: 13.3101 - val_mse: 1.8001\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 11.3884 - mse: 0.0598 - val_loss: 12.7535 - val_mse: 1.7739\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 10.8570 - mse: 0.0622 - val_loss: 12.2163 - val_mse: 1.7324\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 10.3453 - mse: 0.0503 - val_loss: 11.6716 - val_mse: 1.7095\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 9.8526 - mse: 0.0571 - val_loss: 11.1883 - val_mse: 1.6912\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 9.3812 - mse: 0.0537 - val_loss: 10.6840 - val_mse: 1.6602\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 8.9269 - mse: 0.0567 - val_loss: 10.2635 - val_mse: 1.6795\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 8.4919 - mse: 0.0584 - val_loss: 9.7960 - val_mse: 1.6290\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 8.0784 - mse: 0.0575 - val_loss: 9.3730 - val_mse: 1.6169\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 7.6820 - mse: 0.0594 - val_loss: 8.9408 - val_mse: 1.5741\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 7.3015 - mse: 0.0637 - val_loss: 8.5766 - val_mse: 1.5746\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 6.9372 - mse: 0.0659 - val_loss: 8.1646 - val_mse: 1.5263\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Test set MSE: 4169251600.0\n",
      "Test set RMSE: 64569.742\n",
      "Test set RMSPE (%): 31.01251798858374\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, X_test, y_test, log_target=False):\n",
    "    # When predicting, transform the predictions back\n",
    "    y_pred = model.predict(X_test)\n",
    "    if log_target:\n",
    "        y_pred = np.expm1(y_pred).flatten()  # inverse of np.log1p(), make it 1D\n",
    "\n",
    "\n",
    "    # compute the RMSE on the original scale\n",
    "    mse = np.mean(tf.keras.losses.MSE(y_test, y_pred))\n",
    "    print('Test set MSE:', mse)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print('Test set RMSE:', rmse)\n",
    "    rmspe = (np.sqrt(np.mean(np.square((y_test - y_pred) / y_test)))) * 100\n",
    "    print('Test set RMSPE (%):', rmspe)\n",
    "    \n",
    "print('default')\n",
    "model = train_model(X_train=X_train_scaled, y_train=y_train_log, X_valid=X_test_scaled, y_valid=y_valid_log\n",
    "                    , wandb='n')\n",
    "evaluate_model(model, X_test_scaled, y_test, log_target=True)\n",
    "print('run 94')\n",
    "model = train_model(X_train=X_train_scaled, y_train=y_train_log, X_valid=X_test_scaled, y_valid=y_valid_log\n",
    "                    , wandb='n'\n",
    "                    , l1 = .2484\n",
    "                    , l1_l2=True\n",
    "                    , l2 = .3598\n",
    "                    , learning_rate=.00264)\n",
    "evaluate_model(model, X_test_scaled, y_test, log_target=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next things to try\n",
    "- shuffle within the train function so the train, test and validate sets are different each time?\n",
    "- different number and size of layers\n",
    "- different lags (e.i 1 year instead of one quarter) and/or combinations of lags\n",
    "- different static number or make dynamic of epochs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
