{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import random\n",
    "\n",
    "from stable_baselines3 import DQN, PPO, A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# uncomment when SB3 & wandb are working together again\n",
    "# import wandb\n",
    "# from wandb.integration.sb3 import WandbCallback\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_map = {0: 'QB', 1: 'RB', 2: 'WR', 3: 'TE'}\n",
    "\n",
    "player_adp_df = pd.read_csv('adp_2022.csv')\n",
    "player_performance_df = pd.read_csv('player_performance_2022.csv')\n",
    "player_adp_df['Player'] = player_adp_df['Player'].apply(lambda x: ' '.join(x.split()[:2]))\n",
    "player_performance_df['Player'] = player_performance_df['Player'].apply(lambda x: ' '.join(x.split()[:2]))\n",
    "player_df = pd.merge(player_adp_df, player_performance_df, on='Player', how='inner')\n",
    "player_df['Position'] = player_df['Position'].apply(lambda x: x[:2])\n",
    "player_df= player_df[['Player', 'Position', 'AVG', 'FPTS']]\n",
    "\n",
    "\n",
    "class FantasyFootballEnv(gym.Env):\n",
    "    def __init__(self, teams=12, rounds=7, first_round_pick=None):\n",
    "        super(FantasyFootballEnv, self).__init__()\n",
    "        \n",
    "        # Action space: 0: QB, 1: RB, 2: WR, 3: TE\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        \n",
    "        self.observation_space = spaces.Box(low=0, high=rounds, shape=(5,), dtype=np.int32)\n",
    "        \n",
    "        self.teams = teams\n",
    "        self.rounds = rounds\n",
    "        if first_round_pick is not None:\n",
    "            self.first_round_pick = first_round_pick\n",
    "        else:\n",
    "            self.first_round_pick = random.randint(1, teams)\n",
    "        \n",
    "        self.flex_count = 0\n",
    "        \n",
    "        # Other initializations\n",
    "        self.current_round = 1\n",
    "        self.roster = {'QB': [], 'RB': [], 'WR': [], 'TE': []}\n",
    "        self.drafted_players = []\n",
    "            \n",
    "    # def reset(self):\n",
    "    def reset(self, seed=None, first_round_pick=None):\n",
    "        \n",
    "        # https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/#sphx-glr-tutorials-gymnasium-basics-environment-creation-py\n",
    "        # We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        if first_round_pick is not None:\n",
    "            self.first_round_pick = first_round_pick\n",
    "        else:\n",
    "            self.first_round_pick = random.randint(1, self.teams)\n",
    "\n",
    "        self.current_round = 1\n",
    "        self.roster = {'QB': [], 'RB': [], 'WR': [], 'TE': []}\n",
    "        self.drafted_players = []\n",
    "        \n",
    "        # Create the initial observation with the current round and counts for each position\n",
    "        observation = [self.current_round, len(self.roster['QB']), len(self.roster['RB']), len(self.roster['WR']), len(self.roster['TE'])]\n",
    "                \n",
    "        # https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/#sphx-glr-tutorials-gymnasium-basics-environment-creation-py\n",
    "        info = {}\n",
    "                \n",
    "        return observation, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        \n",
    "        penalty = False\n",
    "        done = False\n",
    "        reward = 0\n",
    "        penalty_amt = -10\n",
    "        \n",
    "        selected_position = position_map[action]\n",
    "        pick = self.snake_draft_pick(self.teams, self.current_round, self.first_round_pick)\n",
    "        selected_player = self.draft_player(selected_position, pick)\n",
    "        self.roster[selected_position].append(selected_player)\n",
    "        position_counts = {\n",
    "            'QB': 1,\n",
    "            'RB': 2,\n",
    "            'WR': 2,\n",
    "            'TE': 1,\n",
    "            'FLEX': 1\n",
    "        }\n",
    "        flex_positions = ['RB', 'WR', 'TE']\n",
    "        if selected_position in flex_positions:\n",
    "            if len(self.roster[selected_position]) >= position_counts[selected_position]:\n",
    "                if self.flex_count >= position_counts['FLEX']:\n",
    "                    penalty = True\n",
    "                self.flex_count += 1\n",
    "            # for example qb which isn't a flex\n",
    "            else:\n",
    "                if len(self.roster[selected_position]) >= position_counts[selected_position]:\n",
    "                    penalty = True\n",
    "                    \n",
    "        if penalty:\n",
    "            reward += penalty_amt\n",
    "        \n",
    "        # Create the new observation with the current round and counts for each position\n",
    "        observation = [self.current_round, len(self.roster['QB']), len(self.roster['RB']), len(self.roster['WR']), len(self.roster['TE'])]\n",
    "        \n",
    "        info = {}\n",
    "        info.update({\n",
    "            'round': self.current_round,\n",
    "            'pick': pick,\n",
    "            'selected_position': selected_position,\n",
    "            'selected_player': selected_player,\n",
    "        })\n",
    "        \n",
    "        # Update the current round\n",
    "        self.current_round += 1\n",
    "        \n",
    "        if self.current_round > self.rounds:\n",
    "            done = True\n",
    "            total_points = self.calculate_total_points()\n",
    "            reward += total_points\n",
    "                \n",
    "        # https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/#sphx-glr-tutorials-gymnasium-basics-environment-creation-py\n",
    "        return observation, reward, done, False, info\n",
    "    \n",
    "    # def snake_draft_pick(teams, round, first_round_pick):\n",
    "    def snake_draft_pick(self, teams, round, first_round_pick):\n",
    "        if round % 2 == 1:\n",
    "            return (round - 1) * teams + first_round_pick\n",
    "        else:\n",
    "            return round * teams - first_round_pick + 1\n",
    "\n",
    "\n",
    "    def draft_player(self, selected_position, pick):\n",
    "    \n",
    "        # available_players = player_df[(player_df['Position'].str.startswith(selected_position)) & (player_df['AVG'] >= pick)]\n",
    "        available_players = player_df[(player_df['Position'].str.startswith(selected_position)) & (player_df['AVG'] >= pick) & (~player_df['Player'].isin(self.drafted_players))]  # Filter out players who have already been drafted\n",
    "\n",
    "        \n",
    "        selected_player = available_players.nsmallest(1, 'AVG')\n",
    "        selected_player = selected_player['Player'].iloc[0]\n",
    "        \n",
    "        self.drafted_players.append(selected_player)\n",
    "        \n",
    "        return selected_player\n",
    "    \n",
    "    def calculate_total_points(self):\n",
    "        \n",
    "        # note this method assumes you play your best player not the order they are drafted\n",
    "        \n",
    "        total_points = 0\n",
    "\n",
    "        # Select the highest-scoring QB\n",
    "        qb_points = [player_df[player_df['Player'] == player]['FPTS'].iloc[0] for player in self.roster['QB']]\n",
    "        total_points += max(qb_points, default=0)\n",
    "        # Select the two highest-scoring RBs\n",
    "        rb_points = [player_df[player_df['Player'] == player]['FPTS'].iloc[0] for player in self.roster['RB']]\n",
    "        total_points += sum(sorted(rb_points, reverse=True)[:2])\n",
    "        # Select the two highest-scoring WRs\n",
    "        wr_points = [player_df[player_df['Player'] == player]['FPTS'].iloc[0] for player in self.roster['WR']]\n",
    "        total_points += sum(sorted(wr_points, reverse=True)[:2])\n",
    "        # Select the highest-scoring TE\n",
    "        te_points = [player_df[player_df['Player'] == player]['FPTS'].iloc[0] for player in self.roster['TE']]\n",
    "        total_points += max(te_points, default=0)\n",
    "        # Select the highest-scoring player for the Flex position from the remaining RBs, WRs, and TEs\n",
    "        remaining_rbs = sorted(rb_points[2:], reverse=True)\n",
    "        remaining_wrs = sorted(wr_points[2:], reverse=True)\n",
    "        remaining_tes = sorted(te_points[1:], reverse=True)\n",
    "        remaining_flex = remaining_rbs + remaining_wrs + remaining_tes\n",
    "        # total_points += max(remaining_flex, default=0)\n",
    "        flex_points = max(remaining_flex, default=0)\n",
    "        total_points += flex_points\n",
    "        return total_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_job(model_type\n",
    "                    , use_wandb = 'y', wandb_verbose=2\n",
    "                    , timesteps=1_000_000\n",
    "                    # , policy='MultiInputPolicy'\n",
    "                    , policy='MlpPolicy'\n",
    "                    # should look into mandating that each pick position is considered\n",
    "                    , n_eval_episodes=12\n",
    "                    , vec_envs='n', n_envs=4\n",
    "                    , sb3_model_verbose=0\n",
    "                    # DQN\n",
    "                    , dqn_exploration_final_eps=0.025, dqn_exploration_fraction=0.5\n",
    "                    # PPO\n",
    "                    # https://colab.research.google.com/drive/1GI0WpThwRHbl-Fu2RHfczq6dci5GBDVE#scrollTo=FMdJRrZ4n7xp\n",
    "                    , ppo_n_steps = 1024, ppo_batch_size = 64, ppo_n_epochs = 4, ppo_gamma = 0.999, ppo_gae_lambda = 0.98, ppo_ent_coef = 0.01,\n",
    "                    ):\n",
    "    \n",
    "    config = {\n",
    "    \"policy_type\": policy,\n",
    "    \"total_timesteps\": timesteps,\n",
    "    # \"env_id\": \"NflEnv\",\n",
    "    \"env_id\": \"FantasyFootballEnv\",\n",
    "    }\n",
    "\n",
    "    # https://stable-baselines3.readthedocs.io/en/master/guide/integrations.html\n",
    "    if use_wandb == 'y':\n",
    "        run = wandb.init(\n",
    "            # project=\"sb3_nfl_2\",\n",
    "            project=\"sb3_FantasyFootballEnv\",\n",
    "            config=config,\n",
    "            sync_tensorboard=True\n",
    "        )\n",
    "\n",
    "    # when using multiple environments, the total number of steps taken in counts each step taken in each environment\n",
    "    # if using 4 environments and 400_000 TIMESTEPS, the agent will take a total of 100_000 steps in each environment.\n",
    "    if vec_envs == 'y':\n",
    "        # https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/5_custom_gym_env.ipynb\n",
    "        # received 'ValueError: high is out of bounds for int32' without the seed\n",
    "        # env = make_vec_env(env_id = NflEnv, n_envs=n_envs, seed=1)\n",
    "        # eval_env = make_vec_env(env_id = NflEnv, n_envs=1, seed=1)\n",
    "        env = make_vec_env(env_id = FantasyFootballEnv, n_envs=n_envs, seed=1)\n",
    "        eval_env = make_vec_env(env_id = FantasyFootballEnv, n_envs=1, seed=1)\n",
    "\n",
    "    elif vec_envs == 'n':\n",
    "        # env = NflEnv()\n",
    "        # eval_env = NflEnv()\n",
    "        env = FantasyFootballEnv()\n",
    "        eval_env = FantasyFootballEnv()\n",
    "    \n",
    "    if use_wandb == 'y':\n",
    "        if model_type == 'DQN':\n",
    "            # default values for these parameters are exploration_final_eps=0.05 and exploration_fraction=0.1\n",
    "            # with the default values, the exploration rate will linearly decrease to 0.05 over the first 10% of the timesteps\n",
    "            model = DQN(config[\"policy_type\"], env, verbose=sb3_model_verbose, tensorboard_log=f\"runs/{run.id}\"\n",
    "                        , exploration_final_eps=dqn_exploration_final_eps, exploration_fraction = dqn_exploration_fraction)\n",
    "        elif model_type == 'PPO':\n",
    "            model = PPO(config[\"policy_type\"], env, verbose=sb3_model_verbose, tensorboard_log=f\"runs/{run.id}\"\n",
    "                        , n_steps = ppo_n_steps, batch_size = ppo_batch_size, n_epochs = ppo_n_epochs, gamma = ppo_gamma\n",
    "                        , gae_lambda = ppo_gae_lambda, ent_coef = ppo_ent_coef)\n",
    "        elif model_type == 'A2C':\n",
    "            model = A2C(config[\"policy_type\"], env, verbose=sb3_model_verbose, tensorboard_log=f\"runs/{run.id}\")\n",
    "    else:\n",
    "        if model_type == 'DQN':\n",
    "            model = DQN(config[\"policy_type\"], env, verbose=sb3_model_verbose, exploration_final_eps=dqn_exploration_final_eps\n",
    "                        , exploration_fraction = dqn_exploration_fraction)\n",
    "        elif model_type == 'PPO':\n",
    "            model = PPO(config[\"policy_type\"], env, verbose=sb3_model_verbose\n",
    "                        , n_steps = ppo_n_steps, batch_size = ppo_batch_size, n_epochs = ppo_n_epochs, gamma = ppo_gamma\n",
    "                        , gae_lambda = ppo_gae_lambda, ent_coef = ppo_ent_coef)\n",
    "        elif model_type == 'A2C':\n",
    "            model = A2C(config[\"policy_type\"], env, verbose=sb3_model_verbose)\n",
    "\n",
    "    mean_reward, std_reward = evaluate_policy(model=model, env=eval_env, n_eval_episodes=n_eval_episodes)\n",
    "    print(f\"mean_reward before training:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "    if use_wandb == 'y':\n",
    "        model.learn(\n",
    "            total_timesteps=config[\"total_timesteps\"],\n",
    "            callback=WandbCallback(\n",
    "                model_save_path=f\"models/{run.id}\",\n",
    "                verbose=wandb_verbose,\n",
    "            ),\n",
    "        )\n",
    "        run.finish()\n",
    "    else:\n",
    "        model.learn(total_timesteps=config[\"total_timesteps\"])\n",
    "\n",
    "    mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=n_eval_episodes)\n",
    "    print(f\"mean_reward after training:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "    # parameters_saved = model.get_parameters()\n",
    "    \n",
    "    if vec_envs == 'y':\n",
    "        model.save(f\"models/{model_type}_{timesteps}_vecEnv\")\n",
    "    else:\n",
    "        model.save(f\"models/{model_type}_{timesteps}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\grant\\anaconda3\\envs\\rl\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward before training:873.94 +/- 71.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\grant\\anaconda3\\envs\\rl\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward after training:1517.58 +/- 99.54\n"
     ]
    }
   ],
   "source": [
    "model = run_training_job('PPO',timesteps=1_000_000, use_wandb='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_draft(teams, rounds, first_round_pick, model):\n",
    "    env = FantasyFootballEnv(teams=teams, rounds=rounds, first_round_pick=first_round_pick)\n",
    "    state, info = env.reset(first_round_pick=first_round_pick)\n",
    "    for i in range(rounds):\n",
    "        action, _states = model.predict(state)\n",
    "        action = int(action)  # If action is an array with a single value that can be directly converted to int\n",
    "        new_state, reward, done, placeholder, info = env.step(action)\n",
    "        print(f'Round {info[\"round\"]}, Pick {info[\"pick\"]}, Selected Position {info[\"selected_position\"]}, Selected Player {info[\"selected_player\"]}')\n",
    "        state = new_state\n",
    "        if done:\n",
    "            print(f'Reward: {reward}')\n",
    "\n",
    "def manual_draft(teams, rounds, first_round_pick, actions):\n",
    "    env = FantasyFootballEnv(teams=teams, rounds=rounds, first_round_pick=first_round_pick)\n",
    "    state, info = env.reset()\n",
    "    for action in actions:\n",
    "        new_state, reward, done, placeholder, info = env.step(action)\n",
    "        print(f'Round {info[\"round\"]}, Pick {info[\"pick\"]}, Selected Position {info[\"selected_position\"]}, Selected Player {info[\"selected_player\"]}')\n",
    "        state = new_state\n",
    "        if done:\n",
    "            print(f'Reward: {reward}')\n",
    "\n",
    "teams=12\n",
    "rounds=7\n",
    "# rb first\n",
    "actions = [1, 1, 1, 2, 2, 3, 0]\n",
    "# manual_draft(teams=teams, rounds=rounds, first_round_pick=first_round_pick, actions=actions)\n",
    "# agent_draft(teams=teams, rounds=rounds, first_round_pick=first_round_pick, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first round pick: 1\n",
      "\n",
      "run first manual\n",
      "Round 1, Pick 4, Selected Position RB, Selected Player Derrick Henry\n",
      "Round 2, Pick 21, Selected Position RB, Selected Player Leonard Fournette\n",
      "Round 3, Pick 28, Selected Position RB, Selected Player James Conner\n",
      "Round 4, Pick 45, Selected Position WR, Selected Player Diontae Johnson\n",
      "Round 5, Pick 52, Selected Position WR, Selected Player DK Metcalf\n",
      "Round 6, Pick 69, Selected Position TE, Selected Player Dallas Goedert\n",
      "Round 7, Pick 76, Selected Position QB, Selected Player Dak Prescott\n",
      "Reward: 1300.9\n",
      "\n",
      "agent\n",
      "Round 1, Pick 1, Selected Position TE, Selected Player Travis Kelce\n",
      "Round 2, Pick 24, Selected Position WR, Selected Player Michael Pittman\n",
      "Round 3, Pick 25, Selected Position QB, Selected Player Patrick Mahomes\n",
      "Round 4, Pick 48, Selected Position RB, Selected Player Josh Jacobs\n",
      "Round 5, Pick 49, Selected Position RB, Selected Player AJ Dillon\n",
      "Round 6, Pick 72, Selected Position WR, Selected Player Adam Thielen\n",
      "Round 7, Pick 73, Selected Position RB, Selected Player Dameon Pierce\n",
      "Reward: 1599.0\n"
     ]
    }
   ],
   "source": [
    "first_round_pick=1\n",
    "print(f'first round pick: {first_round_pick}\\n')\n",
    "print('run first manual')\n",
    "manual_draft(teams=teams, rounds=rounds, first_round_pick=first_round_pick, actions=actions)\n",
    "print()\n",
    "print('agent')\n",
    "agent_draft(teams=teams, rounds=rounds, first_round_pick=first_round_pick, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first round pick: 7\n",
      "\n",
      "run first manual\n",
      "Round 1, Pick 1, Selected Position RB, Selected Player Jonathan Taylor\n",
      "Round 2, Pick 24, Selected Position RB, Selected Player Ezekiel Elliott\n",
      "Round 3, Pick 25, Selected Position RB, Selected Player James Conner\n",
      "Round 4, Pick 48, Selected Position WR, Selected Player Diontae Johnson\n",
      "Round 5, Pick 49, Selected Position WR, Selected Player Jaylen Waddle\n",
      "Round 6, Pick 72, Selected Position TE, Selected Player Dawson Knox\n",
      "Round 7, Pick 73, Selected Position QB, Selected Player Dak Prescott\n",
      "Reward: 1216.4\n",
      "\n",
      "agent\n",
      "Round 1, Pick 7, Selected Position TE, Selected Player Travis Kelce\n",
      "Round 2, Pick 18, Selected Position WR, Selected Player Tyreek Hill\n",
      "Round 3, Pick 31, Selected Position QB, Selected Player Patrick Mahomes\n",
      "Round 4, Pick 42, Selected Position RB, Selected Player David Montgomery\n",
      "Round 5, Pick 55, Selected Position WR, Selected Player Jerry Jeudy\n",
      "Round 6, Pick 66, Selected Position RB, Selected Player Clyde Edwards-Helaire\n",
      "Round 7, Pick 79, Selected Position RB, Selected Player Dameon Pierce\n",
      "Reward: 1596.1000000000001\n"
     ]
    }
   ],
   "source": [
    "first_round_pick=7\n",
    "print(f'first round pick: {first_round_pick}\\n')\n",
    "print('run first manual')\n",
    "manual_draft(teams=teams, rounds=rounds, first_round_pick=first_round_pick, actions=actions)\n",
    "print()\n",
    "print('agent')\n",
    "agent_draft(teams=teams, rounds=rounds, first_round_pick=first_round_pick, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first round pick: 12\n",
      "\n",
      "run first manual\n",
      "Round 1, Pick 1, Selected Position RB, Selected Player Jonathan Taylor\n",
      "Round 2, Pick 24, Selected Position RB, Selected Player Ezekiel Elliott\n",
      "Round 3, Pick 25, Selected Position RB, Selected Player James Conner\n",
      "Round 4, Pick 48, Selected Position WR, Selected Player Diontae Johnson\n",
      "Round 5, Pick 49, Selected Position WR, Selected Player Jaylen Waddle\n",
      "Round 6, Pick 72, Selected Position TE, Selected Player Dawson Knox\n",
      "Round 7, Pick 73, Selected Position QB, Selected Player Dak Prescott\n",
      "Reward: 1216.4\n",
      "\n",
      "agent\n",
      "Round 1, Pick 12, Selected Position TE, Selected Player Travis Kelce\n",
      "Round 2, Pick 13, Selected Position WR, Selected Player CeeDee Lamb\n",
      "Round 3, Pick 36, Selected Position QB, Selected Player Justin Herbert\n",
      "Round 4, Pick 37, Selected Position RB, Selected Player Travis Etienne\n",
      "Round 5, Pick 60, Selected Position WR, Selected Player Brandin Cooks\n",
      "Round 6, Pick 61, Selected Position RB, Selected Player Antonio Gibson\n",
      "Round 7, Pick 84, Selected Position RB, Selected Player Devin Singletary\n",
      "Reward: 1412.8\n"
     ]
    }
   ],
   "source": [
    "first_round_pick=12\n",
    "print(f'first round pick: {first_round_pick}\\n')\n",
    "print('run first manual')\n",
    "manual_draft(teams=teams, rounds=rounds, first_round_pick=first_round_pick, actions=actions)\n",
    "print()\n",
    "print('agent')\n",
    "agent_draft(teams=teams, rounds=rounds, first_round_pick=first_round_pick, model=model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
