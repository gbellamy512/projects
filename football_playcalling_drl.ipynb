{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5e3e18f",
   "metadata": {},
   "source": [
    "In this project we'll train agents that choose to run, pass, kick or punt based on the current yards to first, yards to goal, and down.\n",
    "\n",
    "First we need to create a stable baselines3 compliant custom environment (SB3's guide - https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66467bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "from scipy import stats\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "# the function provides the other team's expected points when there is a change of possession.\n",
    "def calc_ep(ydsToGo, ydsToGo_ep_intercept=5.211187389603027, ydsToGo_coef=-0.06155063):\n",
    "    return ydsToGo_ep_intercept + ydsToGo * ydsToGo_coef\n",
    "\n",
    "touchback_ep = calc_ep(80)\n",
    "\n",
    "def normalize_continuous(value, min_value, max_value):\n",
    "    return (value - min_value) / (max_value - min_value)\n",
    "\n",
    "class NflEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self, init_yds_to_goal = 80):\n",
    "        super(NflEnv, self).__init__()\n",
    "\n",
    "        self.action_space = spaces.Discrete(4)  # 0 - run, 1 - pass, 2 - kick, 3 - punt\n",
    "\n",
    "        \n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"yds_to_goal\": spaces.Box(low=0, high=100, shape=(1,), dtype=np.float32),\n",
    "            \"yds_to_first\": spaces.Box(low=0, high=100, shape=(1,), dtype=np.float32),\n",
    "            \"down\": spaces.Discrete(4)\n",
    "        })\n",
    "\n",
    "        \n",
    "        # self.init_yds_to_goal = init_yds_to_goal\n",
    "        self.yds_to_goal = random.triangular(1, 80, 99)\n",
    "        self.init_down = 0\n",
    "        self.init_yds_to_first = 10\n",
    "        \n",
    "        self.comp_percent=.642\n",
    "        self.int_rate=.023\n",
    "        self.sack_rate=.067\n",
    "        self.sack_avg=-5\n",
    "        self.sack_sd=1\n",
    "        self.fmb_rate=.5 / (21.4 + 27.3)\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        prev_down = self.down\n",
    "        prev_yds_to_first = self.yds_to_first\n",
    "        prev_yds_to_goal = self.yds_to_goal\n",
    "        \n",
    "        self.down += 1\n",
    "        score = 0\n",
    "        yds = 0\n",
    "        turnover = False\n",
    "\n",
    "        # kick\n",
    "        if action == 2:\n",
    "            prob = random.rand()\n",
    "            # made fg\n",
    "            # https://www.reddit.com/r/nfl/comments/d4h2r0/kicker_accuracy_accounting_for_distance/\n",
    "            if ((self.yds_to_goal < 60) & (self.yds_to_goal >= 50) & (prob <= .54)) \\\n",
    "                    | ((self.yds_to_goal < 50) & (self.yds_to_goal >= 40) & (prob <= .79)) \\\n",
    "                    | ((self.yds_to_goal < 40) & (self.yds_to_goal >= 30) & (prob <= .90)) \\\n",
    "                    | ((self.yds_to_goal < 30) & (prob <= .95)):\n",
    "                yds = self.yds_to_goal\n",
    "                self.yds_to_goal = 0\n",
    "                self.yds_to_first = 0\n",
    "                score = 3\n",
    "                self.reward = score - touchback_ep\n",
    "            # missed fg\n",
    "            else:\n",
    "                score = 0\n",
    "                self.reward = score - calc_ep(100 - self.yds_to_goal)\n",
    "            self.done = True\n",
    "        # punt\n",
    "        elif action == 3:  \n",
    "            potential_yds = random.normal(loc=45, scale=5)\n",
    "            if potential_yds >= self.yds_to_goal:\n",
    "                touchback = (np.random.rand() < 0.1)\n",
    "                if touchback:\n",
    "                    yds = self.yds_to_goal - 20\n",
    "                    self.yds_to_goal = 20\n",
    "                else:\n",
    "                    yds = self.yds_to_goal - 1\n",
    "                    self.yds_to_goal = 1\n",
    "            else:\n",
    "                yds = potential_yds\n",
    "                self.yds_to_goal = self.yds_to_goal - yds\n",
    "    \n",
    "            self.reward += ((yds / 100) - calc_ep(100 - self.yds_to_goal))\n",
    "            self.done = True\n",
    "\n",
    "        # run\n",
    "        elif action == 0:\n",
    "            yds = stats.lognorm.rvs(s=0.25558500132474415, loc=-15.189614313924665, scale=18.92331909863129, size=1)[0]\n",
    "            # fumble?\n",
    "            if np.random.rand() <= self.fmb_rate:\n",
    "                turnover = True\n",
    "        # pass\n",
    "        elif action == 1:\n",
    "            # sack?\n",
    "            if np.random.rand() <= self.sack_rate:\n",
    "                yds = random.normal(loc=self.sack_avg, scale=self.sack_sd)\n",
    "                # fumble?\n",
    "                if np.random.rand() <= self.fmb_rate:\n",
    "                    turnover = True\n",
    "            else:\n",
    "                if self.yds_to_goal <= 20:\n",
    "                    air_yds = \\\n",
    "                        stats.lognorm.rvs(s=0.18033921317690882, loc=-20.781271075327744, scale=27.083791999969066, size=1)[0]\n",
    "                else:\n",
    "                    air_yds = \\\n",
    "                        stats.lognorm.rvs(s=0.3868256330626365, loc=-11.72666357691477, scale=21.714746429166233, size=1)[0]\n",
    "                prob = np.random.rand()\n",
    "                # completion?\n",
    "                if prob <= self.comp_percent:\n",
    "                    yds = air_yds\n",
    "                    # fumble?\n",
    "                    if np.random.rand() <= self.fmb_rate:\n",
    "                        yds = air_yds\n",
    "                        turnover = True\n",
    "                # int?\n",
    "                elif prob >= (1 - self.int_rate):\n",
    "                    yds = air_yds\n",
    "                    turnover = True\n",
    "                # incomplete\n",
    "                else:\n",
    "                    yds = 0\n",
    "        # if action is not kick or punt\n",
    "        if action in [0, 1]:\n",
    "            self.yds_to_goal = max(self.yds_to_goal - yds, 0)\n",
    "            self.yds_to_first -= yds\n",
    "            # first down?\n",
    "            if self.yds_to_first <= 0:\n",
    "                self.down = self.init_down\n",
    "                self.yds_to_first = min(self.yds_to_goal, self.init_yds_to_first)\n",
    "            # turnover on downs?\n",
    "            elif self.down >= (self.init_down + 4):\n",
    "                turnover = True\n",
    "            # td?\n",
    "            if self.yds_to_goal == 0:\n",
    "                score = 7\n",
    "                self.reward = score - touchback_ep\n",
    "                self.done = True\n",
    "            if turnover == True:\n",
    "                score = 0\n",
    "                # should take touchback into account\n",
    "                self.reward = score - calc_ep(100 - self.yds_to_goal)\n",
    "                self.done = True\n",
    "            # trying a shaping reward here. just kind of winging this, probably needs refined\n",
    "            self.reward += (yds / 100)\n",
    "        \n",
    "        # not sure this is right but I think 5th down (expressed as 4) is breaking things turnover, kick on fourth down \n",
    "        if (self.down == (self.init_down + 4)):\n",
    "            self.down = self.init_down\n",
    "\n",
    "        self.state = {\n",
    "            \"yds_to_goal\": np.array([self.yds_to_goal], dtype=np.float32),\n",
    "            \"yds_to_first\": np.array([self.yds_to_first], dtype=np.float32),\n",
    "            \"down\": self.down  # Represents value 3 (0: 1st down, 1: 2nd down, 2: 3rd down, 3: 4th down)\n",
    "        }\n",
    "        \n",
    "        self.episode_reward += self.reward\n",
    "        \n",
    "        info = {}\n",
    "        info.update({\n",
    "            'prev_down': prev_down,\n",
    "            'prev_yds_to_first': prev_yds_to_first,\n",
    "            'prev_yds_to_goal': prev_yds_to_goal,\n",
    "            'yds': yds,\n",
    "            'turnover': turnover,\n",
    "            'score': score\n",
    "        })\n",
    "        \n",
    "        # https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/#sphx-glr-tutorials-gymnasium-basics-environment-creation-py\n",
    "        return self.state, self.reward, self.done, False, info\n",
    "\n",
    "    def reset(self, seed=None, yds_to_goal=None):\n",
    "\n",
    "        # https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/#sphx-glr-tutorials-gymnasium-basics-environment-creation-py\n",
    "        # We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self.down = self.init_down\n",
    "        self.yds_to_first = self.init_yds_to_first\n",
    "        if yds_to_goal is not None:\n",
    "            self.yds_to_goal = yds_to_goal\n",
    "        else:\n",
    "            self.yds_to_goal = random.triangular(1, 80, 99)\n",
    "        \n",
    "        self.state = {\n",
    "            \"yds_to_goal\": np.array([self.yds_to_goal], dtype=np.float32),\n",
    "            \"yds_to_first\": np.array([self.yds_to_first], dtype=np.float32),\n",
    "            \"down\": self.down  # Represents value 3 (0: 1st down, 1: 2nd down, 2: 3rd down, 3: 4th down)\n",
    "        }\n",
    "        \n",
    "        self.reward = 0\n",
    "        self.episode_reward = 0\n",
    "        self.done = False\n",
    "        \n",
    "        # https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/#sphx-glr-tutorials-gymnasium-basics-environment-creation-py\n",
    "        info = {}\n",
    "        return self.state, info\n",
    "    \n",
    "\n",
    "    \n",
    "    def set_yds_to_goal(self, yds_to_goal):\n",
    "        self.yds_to_goal = yds_to_goal\n",
    "        self.state = {\n",
    "            \"yds_to_goal\": self.yds_to_goal\n",
    "            , \"yds_to_first\": self.yds_to_first\n",
    "            , \"down\": self.down}\n",
    "        \n",
    "        return self.state\n",
    "        \n",
    "    \n",
    "    def pretty_print(self, state, action, info):\n",
    "        down = state[\"down\"]\n",
    "        yds_to_first = state[\"yds_to_first\"][0]\n",
    "        yds_to_goal = state[\"yds_to_goal\"][0]\n",
    "        \n",
    "        yds = info[\"yds\"]\n",
    "        turnover = info[\"turnover\"]\n",
    "        score = info[\"score\"]\n",
    "        prev_down = info[\"prev_down\"]\n",
    "        prev_yds_to_first = info[\"prev_yds_to_first\"]\n",
    "        prev_yds_to_goal = info[\"prev_yds_to_goal\"]\n",
    "\n",
    "        if action == 0:\n",
    "            action = \"Run\"\n",
    "        elif action == 1:\n",
    "            action = \"Pass\"\n",
    "        elif action == 2:\n",
    "            action = \"Kick\"\n",
    "        elif action == 3:\n",
    "            action = \"Punt\"\n",
    "        \n",
    "        print(f\"Down: {prev_down}, Yards to First: {prev_yds_to_first:.1f}, Yards to Goal: {prev_yds_to_goal:.1f}, Action: {action}, Yards Gained: {yds:.1f}, Turnover: {turnover}\")\n",
    "        if turnover: print('Turnover')\n",
    "        if action == 'Kick':\n",
    "            if score == 3:\n",
    "                print('Made Field Goal')\n",
    "            else:\n",
    "                print('Missed Field Goal')\n",
    "        if score == 7: print('Touchdown')\n",
    "            \n",
    "        return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7db71304",
   "metadata": {},
   "source": [
    "Before we train the agents let's first try a custom policy on the environment. We'll use this as a benchmark to see how well the agents learn. Further, let's create two styles (chill & agro) so we can watch them play against each other. We'll want that functionality when we have the agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "084b3543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chill Custom policy mean reward: 3.64 +/- 3.68\n",
      "agro Custom policy mean reward: 3.51 +/- 3.70\n"
     ]
    }
   ],
   "source": [
    "from random import choices\n",
    "\n",
    "def custom_policy(state, style = 'chill'):\n",
    "    down = state['down']\n",
    "    yds_to_first = state['yds_to_first']\n",
    "    yds_to_goal = state['yds_to_goal']\n",
    "\n",
    "    if down == 0:\n",
    "        if style == 'chill':\n",
    "            # run first team\n",
    "            return choices([0, 1], weights=[70, 30], k=1)[0]\n",
    "        elif style == 'agro':\n",
    "            # pass first team\n",
    "            return choices([0, 1], weights=[30, 70], k=1)[0]\n",
    "        \n",
    "    if down == 1:\n",
    "        # heavy run first if ahead of the sticks\n",
    "        if yds_to_first <= (2*10/3):\n",
    "            return choices([0, 1], weights=[85, 15], k=1)[0]\n",
    "        # else pass first\n",
    "        else:\n",
    "            return choices([0, 1], weights=[30, 70], k=1)[0]\n",
    "        \n",
    "    if down == 2:\n",
    "        # passing down if well behind the sticks\n",
    "        if yds_to_first >= (2*10/3):\n",
    "            return 1\n",
    "        # heavy run first if ahead of the sticks\n",
    "        elif yds_to_first <= (10/3):\n",
    "            return choices([0, 1], weights=[85, 15], k=1)[0]\n",
    "        # else (>3.33 and <6.67) pass first\n",
    "        else:\n",
    "            return choices([0, 1], weights=[30, 70], k=1)[0]\n",
    "\n",
    "    # Kick on fourth down if yards to goal is less than 60 yards else same logic as third down\n",
    "    if down == 3:\n",
    "        # agro will always go for it ahead of the sticks\n",
    "        if (style == 'agro') & (yds_to_first <= (10/3)):\n",
    "            return choices([0, 1], weights=[85, 15], k=1)[0]\n",
    "        # kick\n",
    "        elif yds_to_goal < 60:\n",
    "            return 2\n",
    "        # punt\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "def custom_policy(state, style = 'chill'):\n",
    "    down = state['down']\n",
    "    yds_to_first = state['yds_to_first']\n",
    "    yds_to_goal = state['yds_to_goal']\n",
    "\n",
    "    if down == 0:\n",
    "        if style == 'chill':\n",
    "            # run first team\n",
    "            return choices([0, 1], weights=[70, 30], k=1)[0]\n",
    "        elif style == 'agro':\n",
    "            # pass first team\n",
    "            return choices([0, 1], weights=[30, 70], k=1)[0]\n",
    "        \n",
    "    if down == 1:\n",
    "        # heavy run first if ahead of the sticks\n",
    "        if yds_to_first <= (2*10/3):\n",
    "            return choices([0, 1], weights=[85, 15], k=1)[0]\n",
    "        # else pass first\n",
    "        else:\n",
    "            return choices([0, 1], weights=[30, 70], k=1)[0]\n",
    "        \n",
    "    if down == 2:\n",
    "        # passing down if well behind the sticks\n",
    "        if yds_to_first >= (2*10/3):\n",
    "            return 1\n",
    "        # heavy run first if ahead of the sticks\n",
    "        elif yds_to_first <= (10/3):\n",
    "            return choices([0, 1], weights=[85, 15], k=1)[0]\n",
    "        # else (>3.33 and <6.67) pass first\n",
    "        else:\n",
    "            return choices([0, 1], weights=[30, 70], k=1)[0]\n",
    "\n",
    "    # Kick on fourth down if yards to goal is less than 60 yards else same logic as third down\n",
    "    if down == 3:\n",
    "        # agro will always go for it ahead of the sticks\n",
    "        if (style == 'agro') & (yds_to_first <= (10/3)):\n",
    "            return choices([0, 1], weights=[85, 15], k=1)[0]\n",
    "        # kick\n",
    "        elif yds_to_goal < 60:\n",
    "            return 2\n",
    "        # punt\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "\n",
    "def evaluate_custom_policy(env, style='chill', n_episodes=1000, watch=False):\n",
    "    rewards = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "\n",
    "        while not done:\n",
    "            action = custom_policy(state)\n",
    "            state, reward, done, placeholder, info = env.step(np.array([action]))\n",
    "            episode_reward += reward\n",
    "\n",
    "            if watch and episode == n_episodes - 1:\n",
    "                yds = info['yds']\n",
    "                prev_down = info['prev_down']\n",
    "                prev_yds_to_first = info['prev_yds_to_first']\n",
    "                prev_yds_to_goal = info['prev_yds_to_goal']\n",
    "                turnover = info['turnover']\n",
    "                print(f\"Step: {step_count}, Dwn: {prev_down}, to First: {round(float(prev_yds_to_first), 2)}, to Goal: {round(float(prev_yds_to_goal), 2)}, Action: {action}, Yds Gained: {round(float(yds), 2)}, Reward: {round(float(reward), 2)}, Tot Reward: {round(float(episode_reward), 2)}, TO: {turnover}\")\n",
    "\n",
    "            step_count += 1\n",
    "        \n",
    "        rewards.append(episode_reward)\n",
    "\n",
    "    mean_reward = np.mean(rewards)\n",
    "    std_reward = np.std(rewards)\n",
    "\n",
    "    return mean_reward, std_reward\n",
    "\n",
    "n_eval_episodes = 1_000\n",
    "styles = ['chill', 'agro']\n",
    "for style in styles:\n",
    "    env = NflEnv()\n",
    "    mean_reward_custom, std_reward_custom = evaluate_custom_policy(env, style, n_episodes=n_eval_episodes, watch=False)\n",
    "    print(f\"{style} Custom policy mean reward: {mean_reward_custom:.2f} +/- {std_reward_custom:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8532a0e7",
   "metadata": {},
   "source": [
    "Let's watch the two styles play a game against each other, first with text and then with images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9175e77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def draw_field(player_with_ball, starting_yds_to_goal, yds, drive_yds, player_1_score, player_2_score, one_image='n'):\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.barh(0, 100, color='green', height=5)\n",
    "    plt.xlim(0, 100)\n",
    "    plt.ylim(-1, 1)\n",
    "    plt.xticks(range(0, 101, 10))\n",
    "    plt.yticks([])\n",
    "    plt.grid(axis='x')\n",
    "\n",
    "    if player_with_ball == 1:\n",
    "        starting_ball_position = (100 - starting_yds_to_goal)\n",
    "        ball_position = starting_ball_position + drive_yds\n",
    "    elif player_with_ball == 2:\n",
    "        starting_ball_position = starting_yds_to_goal\n",
    "        ball_position = starting_ball_position - drive_yds\n",
    "    \n",
    "    # Plot the ball\n",
    "    plt.plot(ball_position, 0, 'o', markersize=20, color='orange')\n",
    "\n",
    "    plt.text(ball_position, 0.1, f'Play gain: {yds} yards\\nTotal drive: {drive_yds} yards',\n",
    "             horizontalalignment='center', verticalalignment='bottom', color='white', fontsize=12, fontweight='bold')\n",
    "\n",
    "    if player_with_ball == 1:\n",
    "        plt.barh(0, drive_yds, left=starting_ball_position, color='red', height=0.1)\n",
    "    else:  # team 2\n",
    "        plt.barh(0, drive_yds, left=ball_position, color='blue', height=0.1)\n",
    "    \n",
    "    # Add a scoreboard in the top right corner\n",
    "    plt.text(98, 0.8, f'Team 1: {player_1_score}\\nTeam 2: {player_2_score}', \n",
    "             horizontalalignment='right', verticalalignment='top', \n",
    "             bbox=dict(boxstyle=\"square,pad=1\", fc=\"white\", ec=\"black\"), fontsize=14)\n",
    "    \n",
    "    if one_image == 'n':\n",
    "        plt.show()\n",
    "    else:\n",
    "        clear_output(wait=True)  # This will clear the output before showing the new plot\n",
    "        display(plt.gcf())  # This will display the current plot\n",
    "        plt.close()  # This will close the plot so it doesn't get shown again later\n",
    "        time.sleep(0.25)  # wait for quarter second between plays\n",
    "        \n",
    "\n",
    "\n",
    "def play_game(env, possessions=2, style_1='chill', style_2='agro', model_1=None, model_2=None, players = [1, 2], text_viz='y', image_viz='n', one_image='n'):\n",
    "    \n",
    "    starting_yds_to_goal = 80\n",
    "    player_1_score = 0\n",
    "    player_2_score = 0\n",
    "    \n",
    "    if model_1 is not None:\n",
    "        print('player 1 is an agent')\n",
    "    else:\n",
    "        print(f'player 1 is using {style_1} play style')\n",
    "    if model_2 is not None:\n",
    "        print('player 2 is an agent')\n",
    "    else:\n",
    "        print(f'player 2 is using {style_2} play style')\n",
    "    print()\n",
    "\n",
    "    for possessions in range(1, possessions+1):\n",
    "\n",
    "        for player in players:\n",
    "            \n",
    "            if text_viz == 'y':\n",
    "                if player == 1:\n",
    "                    print(f\"player 1 taking possession with {starting_yds_to_goal:.2f} yards to go\")\n",
    "                else:\n",
    "                    print(f\"player 2 taking possession with {starting_yds_to_goal:.2f} yards to go\")\n",
    "            \n",
    "            # state = env.reset(yds_to_goal = starting_yds_to_goal)\n",
    "            state, info = env.reset(yds_to_goal = starting_yds_to_goal)\n",
    "            \n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            \n",
    "            drive_yds = 0\n",
    "\n",
    "            while not done:\n",
    "                state_array = {'yds_to_goal': np.array([state['yds_to_goal']]), 'yds_to_first': np.array([state['yds_to_first']]), 'down': np.array([state['down']])}\n",
    "                if player == 1:\n",
    "                    if model_1 is not None:\n",
    "                        action, _states = model_1.predict(state_array)\n",
    "                    else:\n",
    "                        action = custom_policy(state, style=style_1)\n",
    "                elif player == 2:\n",
    "                    if model_2 is not None:\n",
    "                        action, _states = model_2.predict(state_array)\n",
    "                    else:\n",
    "                        action = custom_policy(state, style=style_2)\n",
    "                state, reward, done, placeholder, info = env.step(np.array([action]))\n",
    "                episode_reward += reward\n",
    "                \n",
    "                score = info[\"score\"]                \n",
    "                if player == 1:\n",
    "                    player_1_score += score\n",
    "                else:\n",
    "                    player_2_score += score\n",
    "                \n",
    "                if text_viz == 'y':\n",
    "                    env.pretty_print(state, action, info)\n",
    "                    \n",
    "                if image_viz == 'y':\n",
    "                    yds = info[\"yds\"]\n",
    "                    drive_yds += yds\n",
    "                    draw_field(player_with_ball=player, starting_yds_to_goal=round(starting_yds_to_goal,1), yds=round(yds,1), drive_yds=round(drive_yds,1), player_1_score=player_1_score, player_2_score=player_2_score, one_image=one_image)\n",
    "                                \n",
    "            # if not made fg or td, probably better way to implement. This doesn't include punt touchbacks.\n",
    "            if score < 3:\n",
    "                starting_yds_to_goal = 100 - state[\"yds_to_goal\"][0]\n",
    "            else:\n",
    "                starting_yds_to_goal = 80\n",
    "            \n",
    "            if text_viz == 'y':\n",
    "                print('Scoreboard:')\n",
    "                print(f\"player 1 - {player_1_score}\")\n",
    "                print(f\"player 2 - {player_2_score}\")\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "185f7165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player 1 is using chill play style\n",
      "player 2 is using agro play style\n",
      "\n",
      "player 1 taking possession with 80.00 yards to go\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 80.0, Action: Run, Yards Gained: 3.6, Turnover: False\n",
      "Down: 1, Yards to First: 6.4, Yards to Goal: 76.4, Action: Run, Yards Gained: 3.9, Turnover: False\n",
      "Down: 2, Yards to First: 2.5, Yards to Goal: 72.5, Action: Run, Yards Gained: -2.6, Turnover: False\n",
      "Down: 3, Yards to First: 5.2, Yards to Goal: 75.2, Action: Punt, Yards Gained: 35.9, Turnover: False\n",
      "Scoreboard:\n",
      "player 1 - 0\n",
      "player 2 - 0\n",
      "\n",
      "player 2 taking possession with 60.71 yards to go\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 60.7, Action: Run, Yards Gained: 5.0, Turnover: False\n",
      "Down: 1, Yards to First: 5.0, Yards to Goal: 55.7, Action: Run, Yards Gained: 4.1, Turnover: False\n",
      "Down: 2, Yards to First: 0.9, Yards to Goal: 51.6, Action: Run, Yards Gained: 13.3, Turnover: False\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 38.3, Action: Run, Yards Gained: 1.1, Turnover: False\n",
      "Down: 1, Yards to First: 8.9, Yards to Goal: 37.2, Action: Pass, Yards Gained: 3.5, Turnover: False\n",
      "Down: 2, Yards to First: 5.4, Yards to Goal: 33.7, Action: Pass, Yards Gained: 10.3, Turnover: False\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 23.4, Action: Pass, Yards Gained: -5.1, Turnover: False\n",
      "Down: 1, Yards to First: 15.1, Yards to Goal: 28.5, Action: Run, Yards Gained: 0.2, Turnover: False\n",
      "Down: 2, Yards to First: 14.9, Yards to Goal: 28.3, Action: Pass, Yards Gained: 0.0, Turnover: False\n",
      "Down: 3, Yards to First: 14.9, Yards to Goal: 28.3, Action: Kick, Yards Gained: 28.3, Turnover: False\n",
      "Made Field Goal\n",
      "Scoreboard:\n",
      "player 1 - 0\n",
      "player 2 - 3\n",
      "\n",
      "player 1 taking possession with 80.00 yards to go\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 80.0, Action: Run, Yards Gained: 3.0, Turnover: False\n",
      "Down: 1, Yards to First: 7.0, Yards to Goal: 77.0, Action: Pass, Yards Gained: 14.5, Turnover: False\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 62.5, Action: Pass, Yards Gained: 14.6, Turnover: False\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 47.8, Action: Run, Yards Gained: 7.6, Turnover: False\n",
      "Down: 1, Yards to First: 2.4, Yards to Goal: 40.3, Action: Run, Yards Gained: -1.0, Turnover: False\n",
      "Down: 2, Yards to First: 3.4, Yards to Goal: 41.2, Action: Run, Yards Gained: 7.0, Turnover: False\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 34.2, Action: Run, Yards Gained: 9.5, Turnover: False\n",
      "Down: 1, Yards to First: 0.5, Yards to Goal: 24.7, Action: Run, Yards Gained: -0.3, Turnover: False\n",
      "Down: 2, Yards to First: 0.8, Yards to Goal: 24.9, Action: Run, Yards Gained: -0.6, Turnover: False\n",
      "Down: 3, Yards to First: 1.4, Yards to Goal: 25.5, Action: Kick, Yards Gained: 25.5, Turnover: False\n",
      "Made Field Goal\n",
      "Scoreboard:\n",
      "player 1 - 3\n",
      "player 2 - 3\n",
      "\n",
      "player 2 taking possession with 80.00 yards to go\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 80.0, Action: Pass, Yards Gained: 12.9, Turnover: False\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 67.1, Action: Run, Yards Gained: -0.1, Turnover: False\n",
      "Down: 1, Yards to First: 10.1, Yards to Goal: 67.3, Action: Pass, Yards Gained: 15.2, Turnover: False\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 52.0, Action: Pass, Yards Gained: 0.0, Turnover: False\n",
      "Down: 1, Yards to First: 10.0, Yards to Goal: 52.0, Action: Pass, Yards Gained: 15.0, Turnover: False\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 37.0, Action: Run, Yards Gained: 2.1, Turnover: False\n",
      "Down: 1, Yards to First: 7.9, Yards to Goal: 34.9, Action: Pass, Yards Gained: 0.0, Turnover: False\n",
      "Down: 2, Yards to First: 7.9, Yards to Goal: 34.9, Action: Pass, Yards Gained: 2.6, Turnover: False\n",
      "Down: 3, Yards to First: 5.3, Yards to Goal: 32.4, Action: Kick, Yards Gained: 32.4, Turnover: False\n",
      "Made Field Goal\n",
      "Scoreboard:\n",
      "player 1 - 3\n",
      "player 2 - 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "play_game(env=NflEnv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdceb30c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# play_game(env=NflEnv(), text_viz='n', image_viz='y')\n",
    "play_game(env=NflEnv(), text_viz='n', image_viz='y', one_image='y')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "313f7603",
   "metadata": {},
   "source": [
    "Now for the fun part, let's use RL to train agents. We'll use stable baselines3 to train and weights & biases to monitor and evaluate our training runs. First, let's import libraries and set hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0c2e4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN, PPO, A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# uncomment when SB3 & wandb are working together again\n",
    "# import wandb\n",
    "# from wandb.integration.sb3 import WandbCallback\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21ffa40",
   "metadata": {},
   "source": [
    "let's first make sure our custom environment will work with SB3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "864effe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_job(model_type\n",
    "                    , use_wandb = 'y', wandb_verbose=2\n",
    "                    , timesteps=1_000_000\n",
    "                    , policy='MultiInputPolicy'\n",
    "                    # needs to be a decent number given starting position is random\n",
    "                    , n_eval_episodes=1_000\n",
    "                    , vec_envs='n', n_envs=4\n",
    "                    , sb3_model_verbose=0\n",
    "                    # DQN\n",
    "                    , dqn_exploration_final_eps=0.025, dqn_exploration_fraction=0.5\n",
    "                    # PPO\n",
    "                    # https://colab.research.google.com/drive/1GI0WpThwRHbl-Fu2RHfczq6dci5GBDVE#scrollTo=FMdJRrZ4n7xp\n",
    "                    , ppo_n_steps = 1024, ppo_batch_size = 64, ppo_n_epochs = 4, ppo_gamma = 0.999, ppo_gae_lambda = 0.98, ppo_ent_coef = 0.01,\n",
    "                    ):\n",
    "    \n",
    "    config = {\n",
    "    \"policy_type\": policy,\n",
    "    \"total_timesteps\": timesteps,\n",
    "    \"env_id\": \"NflEnv\",\n",
    "    }\n",
    "\n",
    "    # https://stable-baselines3.readthedocs.io/en/master/guide/integrations.html\n",
    "    if use_wandb == 'y':\n",
    "        run = wandb.init(\n",
    "            project=\"sb3_nfl_2\",\n",
    "            config=config,\n",
    "            sync_tensorboard=True\n",
    "        )\n",
    "\n",
    "    # when using multiple environments, the total number of steps taken in counts each step taken in each environment\n",
    "    # if using 4 environments and 400_000 TIMESTEPS, the agent will take a total of 100_000 steps in each environment.\n",
    "    if vec_envs == 'y':\n",
    "        # https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/5_custom_gym_env.ipynb\n",
    "        # received 'ValueError: high is out of bounds for int32' without the seed\n",
    "        env = make_vec_env(env_id = NflEnv, n_envs=n_envs, seed=1)\n",
    "        eval_env = make_vec_env(env_id = NflEnv, n_envs=1, seed=1)\n",
    "\n",
    "    elif vec_envs == 'n':\n",
    "        env = NflEnv()\n",
    "        eval_env = NflEnv()\n",
    "    \n",
    "    if use_wandb == 'y':\n",
    "        if model_type == 'DQN':\n",
    "            # default values for these parameters are exploration_final_eps=0.05 and exploration_fraction=0.1\n",
    "            # with the default values, the exploration rate will linearly decrease to 0.05 over the first 10% of the timesteps\n",
    "            model = DQN(config[\"policy_type\"], env, verbose=sb3_model_verbose, tensorboard_log=f\"runs/{run.id}\"\n",
    "                        , exploration_final_eps=dqn_exploration_final_eps, exploration_fraction = dqn_exploration_fraction)\n",
    "        elif model_type == 'PPO':\n",
    "            model = PPO(config[\"policy_type\"], env, verbose=sb3_model_verbose, tensorboard_log=f\"runs/{run.id}\"\n",
    "                        , n_steps = ppo_n_steps, batch_size = ppo_batch_size, n_epochs = ppo_n_epochs, gamma = ppo_gamma\n",
    "                        , gae_lambda = ppo_gae_lambda, ent_coef = ppo_ent_coef)\n",
    "        elif model_type == 'A2C':\n",
    "            model = A2C(config[\"policy_type\"], env, verbose=sb3_model_verbose, tensorboard_log=f\"runs/{run.id}\")\n",
    "    else:\n",
    "        if model_type == 'DQN':\n",
    "            model = DQN(config[\"policy_type\"], env, verbose=sb3_model_verbose, exploration_final_eps=dqn_exploration_final_eps\n",
    "                        , exploration_fraction = dqn_exploration_fraction)\n",
    "        elif model_type == 'PPO':\n",
    "            model = PPO(config[\"policy_type\"], env, verbose=sb3_model_verbose\n",
    "                        , n_steps = ppo_n_steps, batch_size = ppo_batch_size, n_epochs = ppo_n_epochs, gamma = ppo_gamma\n",
    "                        , gae_lambda = ppo_gae_lambda, ent_coef = ppo_ent_coef)\n",
    "        elif model_type == 'A2C':\n",
    "            model = A2C(config[\"policy_type\"], env, verbose=sb3_model_verbose)\n",
    "\n",
    "    mean_reward, std_reward = evaluate_policy(model=model, env=eval_env, n_eval_episodes=n_eval_episodes)\n",
    "    print(f\"mean_reward before training:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "    if use_wandb == 'y':\n",
    "        model.learn(\n",
    "            total_timesteps=config[\"total_timesteps\"],\n",
    "            callback=WandbCallback(\n",
    "                model_save_path=f\"models/{run.id}\",\n",
    "                verbose=wandb_verbose,\n",
    "            ),\n",
    "        )\n",
    "        run.finish()\n",
    "    else:\n",
    "        model.learn(total_timesteps=config[\"total_timesteps\"])\n",
    "\n",
    "    mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=n_eval_episodes)\n",
    "    print(f\"mean_reward after training:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "    # parameters_saved = model.get_parameters()\n",
    "    \n",
    "    if vec_envs == 'y':\n",
    "        model.save(f\"models/{model_type}_{timesteps}_vecEnv\")\n",
    "    else:\n",
    "        model.save(f\"models/{model_type}_{timesteps}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8ee22d",
   "metadata": {},
   "source": [
    "NOTE: as of 7/21/23 there appears to be issue between SB3 and wandb. Need to root cause and fix to enable using wandb during training.\n",
    "use this setup for debugging - https://stable-baselines3.readthedocs.io/en/master/guide/integrations.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5e74b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\grant\\anaconda3\\envs\\rl\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward before training:2.66 +/- 4.99\n",
      "mean_reward after training:3.94 +/- 4.56\n"
     ]
    }
   ],
   "source": [
    "dqn_model = run_training_job('DQN',timesteps=1_000_000, use_wandb='n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b6075a9",
   "metadata": {},
   "source": [
    "Let's watch DQN and PPO play eachother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48f50a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\grant\\anaconda3\\envs\\rl\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward before training:0.93 +/- 2.77\n",
      "mean_reward after training:3.77 +/- 4.99\n"
     ]
    }
   ],
   "source": [
    "ppo_model = run_training_job('PPO',timesteps=1_000_000, use_wandb='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e00a1c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player 1 is an agent\n",
      "player 2 is an agent\n",
      "\n",
      "player 1 taking possession with 80.00 yards to go\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 80.0, Action: Pass, Yards Gained: 13.5, Turnover: False\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 66.5, Action: Pass, Yards Gained: -5.2, Turnover: False\n",
      "Down: 1, Yards to First: 15.2, Yards to Goal: 71.7, Action: Pass, Yards Gained: 0.4, Turnover: False\n",
      "Down: 2, Yards to First: 14.8, Yards to Goal: 71.3, Action: Pass, Yards Gained: 9.7, Turnover: False\n",
      "Down: 3, Yards to First: 5.0, Yards to Goal: 61.6, Action: Pass, Yards Gained: 0.0, Turnover: True\n",
      "Turnover\n",
      "Scoreboard:\n",
      "player 1 - 0\n",
      "player 2 - 0\n",
      "\n",
      "player 2 taking possession with 38.43 yards to go\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 38.4, Action: Run, Yards Gained: 9.1, Turnover: False\n",
      "Down: 1, Yards to First: 0.9, Yards to Goal: 29.3, Action: Pass, Yards Gained: 53.5, Turnover: False\n",
      "Touchdown\n",
      "Scoreboard:\n",
      "player 1 - 0\n",
      "player 2 - 7\n",
      "\n",
      "player 1 taking possession with 80.00 yards to go\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 80.0, Action: Pass, Yards Gained: 12.9, Turnover: False\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 67.1, Action: Pass, Yards Gained: 9.2, Turnover: False\n",
      "Down: 1, Yards to First: 0.8, Yards to Goal: 57.9, Action: Run, Yards Gained: 1.5, Turnover: False\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 56.4, Action: Pass, Yards Gained: 0.0, Turnover: False\n",
      "Down: 1, Yards to First: 10.0, Yards to Goal: 56.4, Action: Pass, Yards Gained: 5.7, Turnover: False\n",
      "Down: 2, Yards to First: 4.3, Yards to Goal: 50.7, Action: Pass, Yards Gained: 13.5, Turnover: False\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 37.2, Action: Pass, Yards Gained: 38.4, Turnover: False\n",
      "Touchdown\n",
      "Scoreboard:\n",
      "player 1 - 7\n",
      "player 2 - 7\n",
      "\n",
      "player 2 taking possession with 80.00 yards to go\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 80.0, Action: Run, Yards Gained: 8.8, Turnover: False\n",
      "Down: 1, Yards to First: 1.2, Yards to Goal: 71.2, Action: Pass, Yards Gained: -4.5, Turnover: False\n",
      "Down: 2, Yards to First: 5.6, Yards to Goal: 75.6, Action: Run, Yards Gained: 8.5, Turnover: False\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 67.1, Action: Run, Yards Gained: 3.7, Turnover: False\n",
      "Down: 1, Yards to First: 6.3, Yards to Goal: 63.4, Action: Run, Yards Gained: 20.5, Turnover: False\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 42.9, Action: Run, Yards Gained: 8.7, Turnover: False\n",
      "Down: 1, Yards to First: 1.3, Yards to Goal: 34.2, Action: Pass, Yards Gained: 17.9, Turnover: False\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 16.3, Action: Run, Yards Gained: 4.8, Turnover: False\n",
      "Down: 1, Yards to First: 5.2, Yards to Goal: 11.5, Action: Run, Yards Gained: -0.1, Turnover: False\n",
      "Down: 2, Yards to First: 5.3, Yards to Goal: 11.6, Action: Run, Yards Gained: 6.1, Turnover: False\n",
      "Down: 0, Yards to First: 5.4, Yards to Goal: 5.4, Action: Run, Yards Gained: 11.0, Turnover: False\n",
      "Touchdown\n",
      "Scoreboard:\n",
      "player 1 - 7\n",
      "player 2 - 14\n",
      "\n",
      "player 1 taking possession with 80.00 yards to go\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 80.0, Action: Pass, Yards Gained: 0.0, Turnover: False\n",
      "Down: 1, Yards to First: 10.0, Yards to Goal: 80.0, Action: Pass, Yards Gained: 7.2, Turnover: True\n",
      "Turnover\n",
      "Scoreboard:\n",
      "player 1 - 7\n",
      "player 2 - 14\n",
      "\n",
      "player 2 taking possession with 27.15 yards to go\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 27.2, Action: Run, Yards Gained: -1.7, Turnover: False\n",
      "Down: 1, Yards to First: 11.7, Yards to Goal: 28.8, Action: Pass, Yards Gained: 32.6, Turnover: False\n",
      "Touchdown\n",
      "Scoreboard:\n",
      "player 1 - 7\n",
      "player 2 - 21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ppo_model = PPO.load('models/PPO_1000000')\n",
    "dqn_model = DQN.load('models/DQN_1000000')\n",
    "play_game(env=NflEnv(), model_1=ppo_model, model_2=dqn_model, possessions=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43cc3ccf",
   "metadata": {},
   "source": [
    "let's see if vectorized environments with more steps improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04021134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{model_type}\n",
      "mean_reward before training:-1.33 +/- 3.04\n",
      "mean_reward after training:4.37 +/- 4.46\n"
     ]
    }
   ],
   "source": [
    "n_envs = 32\n",
    "timesteps=1_000_000 * n_envs\n",
    "model_type = 'PPO'\n",
    "model = run_training_job(model_type=model_type,timesteps=timesteps, vec_envs='y', n_envs=n_envs, use_wandb='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21a4efec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def evaluate_play_style(model, env, num_episodes=1_000):\n",
    "    env = NflEnv()\n",
    "\n",
    "    # Define the columns\n",
    "    columns = ['Game_ID', 'Play_ID', 'yds_to_goal', 'yds_to_first', 'down', 'action', 'reward', 'done', 'new_yds_to_goal', 'new_yds_to_first', 'new_down']\n",
    "\n",
    "    # Initialize the lists\n",
    "    data = {col: [] for col in columns}\n",
    "    \n",
    "    \n",
    "    # Training loop\n",
    "    for i_episode in range(num_episodes):\n",
    "        \n",
    "        state, info = env.reset()\n",
    "        play_id = 0\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        while True:\n",
    "            state_array = {'yds_to_goal': np.array([state['yds_to_goal']]), 'yds_to_first': np.array([state['yds_to_first']]), 'down': np.array([state['down']])}\n",
    "            \n",
    "            action, _states = model.predict(state_array)\n",
    "            new_state, reward, done, placeholder, info = env.step(np.array([action]))\n",
    "            \n",
    "            # Add the play details to the corresponding list\n",
    "            data['Game_ID'].append(i_episode)\n",
    "            data['Play_ID'].append(play_id)\n",
    "            data['yds_to_goal'].append(state[\"yds_to_goal\"][0])\n",
    "            data['yds_to_first'].append(state[\"yds_to_first\"][0])\n",
    "            data['down'].append(state[\"down\"])\n",
    "            # data['action'].append(action)\n",
    "            data['action'].append(action[0])\n",
    "            data['reward'].append(reward)\n",
    "            data['done'].append(done)\n",
    "            data['new_yds_to_goal'].append(new_state[\"yds_to_goal\"][0])\n",
    "            data['new_yds_to_first'].append(new_state[\"yds_to_first\"][0])\n",
    "            data['new_down'].append(new_state[\"down\"])\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = new_state\n",
    "            play_id += 1\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Pivot table to show percentages of actions taken on each down\n",
    "    pivot_df = df.pivot_table(index='down', columns='action', aggfunc='size', fill_value=0)\n",
    "\n",
    "    # Normalize the values (convert counts to percentages)\n",
    "    pivot_df = pivot_df.div(pivot_df.sum(axis=1), axis=0)\n",
    "\n",
    "    # Display the result\n",
    "    print(pivot_df)\n",
    "\n",
    "    df_redzone = df[df['yds_to_goal'] <= 20]\n",
    "\n",
    "    # Do the same for redzone actions\n",
    "    pivot_df_redzone = df_redzone.pivot_table(index='down', columns='action', aggfunc='size', fill_value=0)\n",
    "\n",
    "    # Normalize the values (convert counts to percentages)\n",
    "    pivot_df_redzone = pivot_df_redzone.div(pivot_df_redzone.sum(axis=1), axis=0)\n",
    "\n",
    "    # Display the result\n",
    "    print(pivot_df_redzone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0f9e62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action         0         1         2         3\n",
      "down                                          \n",
      "0       0.952325  0.033885  0.009062  0.004728\n",
      "1       0.760880  0.227642  0.005739  0.005739\n",
      "2       0.619436  0.362760  0.010386  0.007418\n",
      "3       0.521795  0.016667  0.288462  0.173077\n",
      "action         0         1         2         3\n",
      "down                                          \n",
      "0       0.844402  0.136622  0.013283  0.005693\n",
      "1       0.940678  0.055085  0.002119  0.002119\n",
      "2       0.833887  0.152824  0.006645  0.006645\n",
      "3       0.587097  0.000000  0.400000  0.012903\n"
     ]
    }
   ],
   "source": [
    "model = DQN.load(\"models/DQN_1000000\")\n",
    "evaluate_play_style(model, NflEnv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0ed9fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action         0         1\n",
      "down                      \n",
      "0       0.337379  0.662621\n",
      "1       0.413359  0.586641\n",
      "2       0.461169  0.538831\n",
      "3       0.432877  0.567123\n",
      "action         0         1\n",
      "down                      \n",
      "0       0.995495  0.004505\n",
      "1       0.929432  0.070568\n",
      "2       0.905028  0.094972\n",
      "3       0.865217  0.134783\n"
     ]
    }
   ],
   "source": [
    "model = PPO.load(\"models/PPO_1000000\")\n",
    "evaluate_play_style(model, NflEnv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc35c6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action         0         1         2         3\n",
      "down                                          \n",
      "0       0.449745  0.550255  0.000000  0.000000\n",
      "1       0.493280  0.506720  0.000000  0.000000\n",
      "2       0.479115  0.520885  0.000000  0.000000\n",
      "3       0.352770  0.088921  0.345481  0.212828\n",
      "action         0         1        2\n",
      "down                               \n",
      "0       0.995261  0.004739  0.00000\n",
      "1       0.941818  0.058182  0.00000\n",
      "2       0.909859  0.090141  0.00000\n",
      "3       0.587940  0.000000  0.41206\n"
     ]
    }
   ],
   "source": [
    "model = PPO.load(\"models/PPO_32000000_vecEnv\")\n",
    "evaluate_play_style(model, NflEnv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ff7bd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player 1 is an agent\n",
      "player 2 is an agent\n",
      "\n",
      "player 1 taking possession with 80.00 yards to go\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 80.0, Action: Pass, Yards Gained: 0.0, Turnover: False\n",
      "Down: 1, Yards to First: 10.0, Yards to Goal: 80.0, Action: Pass, Yards Gained: 2.3, Turnover: False\n",
      "Down: 2, Yards to First: 7.7, Yards to Goal: 77.7, Action: Pass, Yards Gained: 11.7, Turnover: False\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 66.0, Action: Pass, Yards Gained: 0.0, Turnover: False\n",
      "Down: 1, Yards to First: 10.0, Yards to Goal: 66.0, Action: Pass, Yards Gained: 15.3, Turnover: False\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 50.7, Action: Pass, Yards Gained: 21.6, Turnover: False\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 29.1, Action: Run, Yards Gained: 4.4, Turnover: False\n",
      "Down: 1, Yards to First: 5.6, Yards to Goal: 24.8, Action: Run, Yards Gained: 2.1, Turnover: False\n",
      "Down: 2, Yards to First: 3.6, Yards to Goal: 22.7, Action: Run, Yards Gained: 5.3, Turnover: False\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 17.4, Action: Run, Yards Gained: 3.2, Turnover: False\n",
      "Down: 1, Yards to First: 6.8, Yards to Goal: 14.1, Action: Run, Yards Gained: 8.8, Turnover: False\n",
      "Down: 0, Yards to First: 5.3, Yards to Goal: 5.3, Action: Run, Yards Gained: 10.1, Turnover: False\n",
      "Touchdown\n",
      "Scoreboard:\n",
      "player 1 - 7\n",
      "player 2 - 0\n",
      "\n",
      "player 2 taking possession with 80.00 yards to go\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 80.0, Action: Pass, Yards Gained: 4.8, Turnover: False\n",
      "Down: 1, Yards to First: 5.2, Yards to Goal: 75.2, Action: Run, Yards Gained: 2.1, Turnover: False\n",
      "Down: 2, Yards to First: 3.1, Yards to Goal: 73.1, Action: Run, Yards Gained: 2.1, Turnover: False\n",
      "Down: 3, Yards to First: 1.0, Yards to Goal: 71.0, Action: Run, Yards Gained: 3.7, Turnover: False\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 67.3, Action: Pass, Yards Gained: 14.7, Turnover: False\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 52.6, Action: Pass, Yards Gained: 8.0, Turnover: False\n",
      "Down: 1, Yards to First: 2.0, Yards to Goal: 44.7, Action: Run, Yards Gained: -2.6, Turnover: False\n",
      "Down: 2, Yards to First: 4.7, Yards to Goal: 47.3, Action: Run, Yards Gained: -2.2, Turnover: False\n",
      "Down: 3, Yards to First: 6.9, Yards to Goal: 49.5, Action: Pass, Yards Gained: 14.6, Turnover: False\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 34.8, Action: Run, Yards Gained: -2.3, Turnover: False\n",
      "Down: 1, Yards to First: 12.3, Yards to Goal: 37.1, Action: Pass, Yards Gained: 20.6, Turnover: False\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 16.5, Action: Run, Yards Gained: 5.2, Turnover: False\n",
      "Down: 1, Yards to First: 4.8, Yards to Goal: 11.3, Action: Run, Yards Gained: -5.8, Turnover: False\n",
      "Down: 2, Yards to First: 10.6, Yards to Goal: 17.1, Action: Pass, Yards Gained: 12.7, Turnover: False\n",
      "Down: 0, Yards to First: 4.4, Yards to Goal: 4.4, Action: Run, Yards Gained: 7.6, Turnover: False\n",
      "Touchdown\n",
      "Scoreboard:\n",
      "player 1 - 7\n",
      "player 2 - 7\n",
      "\n",
      "player 1 taking possession with 80.00 yards to go\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 80.0, Action: Pass, Yards Gained: 0.0, Turnover: False\n",
      "Down: 1, Yards to First: 10.0, Yards to Goal: 80.0, Action: Pass, Yards Gained: 0.2, Turnover: False\n",
      "Down: 2, Yards to First: 9.8, Yards to Goal: 79.8, Action: Pass, Yards Gained: 18.6, Turnover: False\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 61.2, Action: Pass, Yards Gained: 15.9, Turnover: False\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 45.3, Action: Pass, Yards Gained: -1.2, Turnover: False\n",
      "Down: 1, Yards to First: 11.2, Yards to Goal: 46.5, Action: Pass, Yards Gained: 12.2, Turnover: False\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 34.2, Action: Run, Yards Gained: 9.1, Turnover: False\n",
      "Down: 1, Yards to First: 0.9, Yards to Goal: 25.1, Action: Run, Yards Gained: 2.4, Turnover: False\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 22.7, Action: Run, Yards Gained: 17.7, Turnover: False\n",
      "Down: 0, Yards to First: 5.1, Yards to Goal: 5.1, Action: Run, Yards Gained: 1.8, Turnover: False\n",
      "Down: 1, Yards to First: 3.3, Yards to Goal: 3.3, Action: Run, Yards Gained: 0.5, Turnover: False\n",
      "Down: 2, Yards to First: 2.8, Yards to Goal: 2.8, Action: Run, Yards Gained: 9.6, Turnover: False\n",
      "Touchdown\n",
      "Scoreboard:\n",
      "player 1 - 14\n",
      "player 2 - 7\n",
      "\n",
      "player 2 taking possession with 80.00 yards to go\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 80.0, Action: Pass, Yards Gained: 0.0, Turnover: False\n",
      "Down: 1, Yards to First: 10.0, Yards to Goal: 80.0, Action: Pass, Yards Gained: 0.9, Turnover: False\n",
      "Down: 2, Yards to First: 9.1, Yards to Goal: 79.1, Action: Pass, Yards Gained: 7.9, Turnover: False\n",
      "Down: 3, Yards to First: 1.2, Yards to Goal: 71.2, Action: Run, Yards Gained: 2.9, Turnover: False\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 68.4, Action: Pass, Yards Gained: 9.6, Turnover: False\n",
      "Down: 1, Yards to First: 0.4, Yards to Goal: 58.7, Action: Run, Yards Gained: 4.7, Turnover: False\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 54.0, Action: Pass, Yards Gained: 12.9, Turnover: False\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 41.1, Action: Pass, Yards Gained: -6.2, Turnover: False\n",
      "Down: 1, Yards to First: 16.2, Yards to Goal: 47.3, Action: Pass, Yards Gained: 0.0, Turnover: False\n",
      "Down: 2, Yards to First: 16.2, Yards to Goal: 47.3, Action: Pass, Yards Gained: 16.8, Turnover: False\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 30.5, Action: Run, Yards Gained: 5.2, Turnover: False\n",
      "Down: 1, Yards to First: 4.8, Yards to Goal: 25.3, Action: Run, Yards Gained: 6.3, Turnover: False\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 19.1, Action: Run, Yards Gained: 8.3, Turnover: False\n",
      "Down: 1, Yards to First: 1.7, Yards to Goal: 10.8, Action: Run, Yards Gained: 2.8, Turnover: False\n",
      "Down: 0, Yards to First: 8.0, Yards to Goal: 8.0, Action: Run, Yards Gained: 6.7, Turnover: False\n",
      "Down: 1, Yards to First: 1.3, Yards to Goal: 1.3, Action: Run, Yards Gained: 6.4, Turnover: False\n",
      "Touchdown\n",
      "Scoreboard:\n",
      "player 1 - 14\n",
      "player 2 - 14\n",
      "\n",
      "player 1 taking possession with 80.00 yards to go\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 80.0, Action: Pass, Yards Gained: 16.9, Turnover: False\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 63.1, Action: Pass, Yards Gained: 0.0, Turnover: False\n",
      "Down: 1, Yards to First: 10.0, Yards to Goal: 63.1, Action: Pass, Yards Gained: 18.9, Turnover: False\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 44.2, Action: Pass, Yards Gained: 6.9, Turnover: False\n",
      "Down: 1, Yards to First: 3.1, Yards to Goal: 37.3, Action: Run, Yards Gained: 4.3, Turnover: False\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 33.0, Action: Run, Yards Gained: -0.1, Turnover: False\n",
      "Down: 1, Yards to First: 10.1, Yards to Goal: 33.1, Action: Pass, Yards Gained: 17.2, Turnover: False\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 15.9, Action: Run, Yards Gained: 4.5, Turnover: False\n",
      "Down: 1, Yards to First: 5.5, Yards to Goal: 11.3, Action: Run, Yards Gained: 8.5, Turnover: False\n",
      "Down: 0, Yards to First: 2.8, Yards to Goal: 2.8, Action: Run, Yards Gained: 4.0, Turnover: False\n",
      "Touchdown\n",
      "Scoreboard:\n",
      "player 1 - 21\n",
      "player 2 - 14\n",
      "\n",
      "player 2 taking possession with 80.00 yards to go\n",
      "Down: 0, Yards to First: 10.0, Yards to Goal: 80.0, Action: Pass, Yards Gained: 4.8, Turnover: True\n",
      "Turnover\n",
      "Scoreboard:\n",
      "player 1 - 21\n",
      "player 2 - 14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_1 = PPO.load(f\"models/PPO_1000000\")\n",
    "model_2 = PPO.load(f\"models/PPO_32000000_vecEnv\")\n",
    "play_game(env=NflEnv(), model_1=model, model_2=model, possessions=3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
